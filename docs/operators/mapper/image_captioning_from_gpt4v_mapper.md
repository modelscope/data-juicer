# image_captioning_from_gpt4v_mapper

Generates text captions for images using the GPT-4 Vision model.

This operator generates text based on the provided images and specified parameters. It supports different modes of text generation, including 'reasoning', 'description', 'conversation', and 'custom'. The generated text can be added to the original sample or replace it, depending on the `keep_original_sample` parameter. The operator uses a Hugging Face tokenizer and the GPT-4 Vision API to generate the text. The `any_or_all` parameter determines whether all or any of the images in a sample must meet the generation criteria for the sample to be kept. If `user_prompt_key` is set, it will use the prompt from the sample; otherwise, it will use the `user_prompt` parameter. If both are set, `user_prompt_key` takes precedence.

ä½¿ç”¨ GPT-4 Vision æ¨¡å‹ä¸ºå›¾åƒç”Ÿæˆæ–‡æœ¬æè¿°ã€‚

è¯¥ç®—å­æ ¹æ®æä¾›çš„å›¾åƒå’ŒæŒ‡å®šçš„å‚æ•°ç”Ÿæˆæ–‡æœ¬ã€‚å®ƒæ”¯æŒä¸åŒçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å¼ï¼ŒåŒ…æ‹¬'reasoning'ã€'description'ã€'conversation'å’Œ'custom'ã€‚æ ¹æ®`keep_original_sample`å‚æ•°ï¼Œç”Ÿæˆçš„æ–‡æœ¬å¯ä»¥æ·»åŠ åˆ°åŸå§‹æ ·æœ¬ä¸­æˆ–æ›¿æ¢å®ƒã€‚è¯¥ç®—å­ä½¿ç”¨ Hugging Face çš„ tokenizer å’Œ GPT-4 Vision API ç”Ÿæˆæ–‡æœ¬ã€‚`any_or_all` å‚æ•°å†³å®šæ ·æœ¬ä¸­çš„æ‰€æœ‰æˆ–ä»»ä½•å›¾åƒæ˜¯å¦å¿…é¡»æ»¡è¶³ç”Ÿæˆæ¡ä»¶æ‰èƒ½ä¿ç•™æ ·æœ¬ã€‚å¦‚æœè®¾ç½®äº†`user_prompt_key`ï¼Œåˆ™ä¼šä½¿ç”¨æ ·æœ¬ä¸­çš„æç¤ºï¼›å¦åˆ™ï¼Œå°†ä½¿ç”¨`user_prompt`å‚æ•°ã€‚å¦‚æœä¸¤è€…éƒ½è®¾ç½®ï¼Œåˆ™`user_prompt_key`ä¼˜å…ˆã€‚

Type ç®—å­ç±»å‹: **mapper**

Tags æ ‡ç­¾: cpu, multimodal

## ğŸ”§ Parameter Configuration å‚æ•°é…ç½®
| name å‚æ•°å | type ç±»å‹ | default é»˜è®¤å€¼ | desc è¯´æ˜ |
|--------|------|--------|------|
| `mode` | <class 'str'> | `'description'` | mode of text generated from images, can be one of ['reasoning', 'description', 'conversation', 'custom'] |
| `api_key` | <class 'str'> | `''` | the API key to authenticate the request. |
| `max_token` | <class 'int'> | `500` | the maximum number of tokens to generate. Default is 500. |
| `temperature` | typing.Annotated[float, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=0), Le(le=1)])] | `1.0` | controls the randomness of the output (range from 0 to 1). Default is 0. |
| `system_prompt` | <class 'str'> | `''` | a string prompt used to set the context of a conversation and provide global guidance or rules for the gpt4-vision so that it can  generate responses in the expected way. If `mode` set to `custom`, the parameter will be used. |
| `user_prompt` | <class 'str'> | `''` | a string prompt to guide the generation of gpt4-vision for each samples. It's "" in default, which means no prompt provided. |
| `user_prompt_key` | typing.Optional[str] | `None` | the key name of fields in samples to store prompts for each sample. It's used for set different prompts for different samples. If it's none, use prompt in parameter "prompt". It's None in default. |
| `keep_original_sample` | <class 'bool'> | `True` | whether to keep the original sample. If it's set to False, there will be only generated text in the final datasets and the original text will be removed. It's True in default. |
| `any_or_all` | <class 'str'> | `'any'` | keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition. |
| `args` |  | `''` | extra args |
| `kwargs` |  | `''` | extra args |

## ğŸ“Š Effect demonstration æ•ˆæœæ¼”ç¤º
not available æš‚æ— 

## ğŸ”— related links ç›¸å…³é“¾æ¥
- [source code æºä»£ç ](../../../data_juicer/ops/mapper/image_captioning_from_gpt4v_mapper.py)
- [unit test å•å…ƒæµ‹è¯•](../../../tests/ops/mapper/test_image_captioning_from_gpt4v_mapper.py)
- [Return operator list è¿”å›ç®—å­åˆ—è¡¨](../../Operators.md)