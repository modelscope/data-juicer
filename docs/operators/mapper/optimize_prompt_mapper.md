# optimize_prompt_mapper


Mapper to optimize prompts based on the existing ones. This OP will use the existing prompts in the same batch and newly optimized prompts as the examples to optimize the next ones.

Reference: https://doc.agentscope.io/v0/en/build_tutorial/prompt_optimization.html


ç”¨äºæ ¹æ®ç°æœ‰æç¤ºè¿›è¡Œä¼˜åŒ–çš„æ˜ å°„å™¨ã€‚æ­¤ç®—å­å°†ä½¿ç”¨åŒä¸€æ‰¹æ¬¡ä¸­çš„ç°æœ‰æç¤ºå’Œæ–°ä¼˜åŒ–çš„æç¤ºä½œä¸ºç¤ºä¾‹æ¥ä¼˜åŒ–åç»­çš„æç¤ºã€‚

å‚è€ƒï¼šhttps://doc.agentscope.io/v0/en/build_tutorial/prompt_optimization.html


*****

Type ç®—å­ç±»å‹: **mapper**

Tags æ ‡ç­¾: cpu, vllm, hf, api

## ğŸ”§ Parameter Configuration å‚æ•°é…ç½®
| name å‚æ•°å | type ç±»å‹ | default é»˜è®¤å€¼ | desc è¯´æ˜ |
|--------|------|--------|------|
| `api_or_hf_model` | <class 'str'> | `'Qwen/Qwen2.5-7B-Instruct'` | API or huggingface model name. |
| `gen_num` | typing.Annotated[int, Gt(gt=0)] | `3` | The number of new prompts to generate. |
| `max_example_num` | typing.Annotated[int, Gt(gt=0)] | `3` |  |
| `keep_original_sample` | <class 'bool'> | `True` | whether to keep the original sample. If it's set to False, there will be only generated texts in the final datasets and the original texts will be removed. It's True in default. |
| `retry_num` | <class 'int'> | `3` | how many times to retry to generate the prompt if the parsed generated prompt is empty. It's 3 in default. |
| `api_endpoint` | typing.Optional[str] | `None` | URL endpoint for the API. |
| `response_path` | typing.Optional[str] | `None` | Path to extract content from the API response. Defaults to 'choices.0.message.content'. |
| `system_prompt` | typing.Optional[str] | `None` | System prompt for guiding the generation task. |
| `input_template` | typing.Optional[str] | `None` | Template for building the input prompt. It must include one placeholder '{}', which will be replaced by `example_num` formatted examples defined by `example_template`. |
| `example_template` | typing.Optional[str] | `None` | Template for formatting one prompt example. It must include one placeholder '{}', which will be replaced by one formatted prompt. |
| `prompt_template` | typing.Optional[str] | `None` | Template for formatting a single prompt within each example. Must include two placeholders '{}' for the question and answer. |
| `output_pattern` | typing.Optional[str] | `None` | Regular expression pattern to extract questions and answers from model response. |
| `enable_vllm` | <class 'bool'> | `False` | Whether to use vllm for inference acceleration. |
| `is_hf_model` | <class 'bool'> | `False` | If true, use Transformers for loading hugging face or local llm. |
| `model_params` | typing.Optional[typing.Dict] | `None` | Parameters for initializing the model. |
| `sampling_params` | typing.Optional[typing.Dict] | `None` | Sampling parameters for text generation. e.g {'temperature': 0.9, 'top_p': 0.95} |
| `kwargs` |  | `''` | Extra keyword arguments. |

## ğŸ“Š Effect demonstration æ•ˆæœæ¼”ç¤º
not available æš‚æ— 

## ğŸ”— related links ç›¸å…³é“¾æ¥
- [source code æºä»£ç ](../../../data_juicer/ops/mapper/optimize_prompt_mapper.py)
- [unit test å•å…ƒæµ‹è¯•](../../../tests/ops/mapper/test_optimize_prompt_mapper.py)
- [Return operator list è¿”å›ç®—å­åˆ—è¡¨](../../Operators.md)