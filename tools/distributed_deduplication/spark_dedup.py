import sys
import time
from typing import Optional

import fire
from loguru import logger
from pyspark.ml.feature import HashingTF, MinHashLSH, Tokenizer
from pyspark.sql import functions as F
from pyspark.sql.functions import posexplode

from tools.distributed_deduplication.dedup_utils import (
    find_components,
    generate_edges,
    init_spark,
)
from tools.quality_classifier.qc_utils import (
    export_result,
    load_dataset,
    tokenize_dataset,
)


@logger.catch
def dedup_dataset(
    dataset_path: str,
    result_path: str,
    tokenizer: Optional[str] = None,
    num_features: int = 1047576,
    num_hashtables: int = 10,
    text_key: str = "text",
    master_url: Optional[str] = None,
):
    """
    Perform fuzzy text deduplication on the given dataset.
    :param dataset_path: the path to the dataset to perform deduplication,
        The suffix of the path should be one of the json, jsonl, parquet.
    :param result_path: the path to store the predicted result dataset.
    :param tokenizer: what tokenizer to use to tokenize texts. It's None in
        default, which means using the standard Tokenizer of PySpark. You can
        use one of ["zh.sp.model", "code.sp.model"] we provided, or you can set
        it to the path to your own sentencepiece model.
    :param num_features: the number of features that HashingTF generates.
        Default with 1047576 as mentioned in megatron-turing-nlg paper.
    :param num_hashtables: the number of hashes used in MinHashLSH.
        Default with 10 hashes as mentioned in the GPT3 paper.
    :param text_key: the field key name to hold texts to be classified. It's
        "text" in default.
    :param master_url: the master url for spark config. Default is None.
        If None, then run with local[*].
    """
    # for inited cluster,
    # provide master url such as "spark://master:7077"
    spark = init_spark(master_url=master_url)
    ds = load_dataset(spark, dataset_path, text_key=text_key)
    ds = ds.withColumn("id", F.monotonically_increasing_id()).cache()
    df = ds

    if tokenizer:
        ds = tokenize_dataset(ds, tokenizer)
    else:
        ds = Tokenizer(inputCol="text", outputCol="words").transform(ds)

    hashingTF = HashingTF(inputCol="words", outputCol="features", numFeatures=num_features)
    ds = hashingTF.transform(ds)

    minHash = MinHashLSH(inputCol="features", outputCol="hashes", numHashTables=num_hashtables)
    model = minHash.fit(ds)

    ds = model.transform(ds)

    ds = ds.select("id", posexplode("hashes").alias("band_idx", "hash_vector"))

    record = ds.rdd.map(lambda x: (x["band_idx"], int(x["hash_vector"][0]), x["id"]))

    edges = (
        record.groupBy(lambda x: (x[0], x[1]))
        .flatMap(lambda x: generate_edges([i[2] for i in x[1]]))
        .distinct()
        .cache()
    )

    results = find_components(edges)
    if len(results) == 0:
        logger.info("No components found.")
        sys.exit(0)

    components = spark.createDataFrame(results, schema=["id", "component"]).sort(["component", "id"])
    components.show()
    df = df.join(components, on="id", how="left")
    df = df.filter(F.col("component").isNull()).drop("id", "component").cache()
    export_result(df, result_path)


if __name__ == "__main__":
    stime = time.time()
    fire.Fire(dedup_dataset)
    etime = time.time()
    logger.info(f"Execution Done, Total time {etime - stime}")
