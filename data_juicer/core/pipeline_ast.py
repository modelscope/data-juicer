# standard library imports
import argparse
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional

# third party imports
import yaml


class OpType(Enum):
    """Types of operations in the pipeline."""
    ROOT = 'root'
    MAPPER = 'mapper'
    FILTER = 'filter'
    DEDUPLICATOR = 'deduplicator'
    SELECTOR = 'selector'
    GROUPER = 'grouper'
    AGGREGATOR = 'aggregator'


@dataclass
class OpNode:
    """Node in the pipeline AST representing an operation."""
    name: str
    op_type: OpType
    config: Dict[str, Any]
    children: List['OpNode'] = None
    parent: Optional['OpNode'] = None

    def __post_init__(self):
        if self.children is None:
            self.children = []

    def add_child(self, child: 'OpNode'):
        """Add a child node to this operation."""
        child.parent = self
        self.children.append(child)

    def to_dict(self) -> Dict[str, Any]:
        """Convert the node to a dictionary representation."""
        return {
            'name': self.name,
            'type': self.op_type.value,
            'config': self.config,
            'children': [child.to_dict() for child in self.children]
        }


class PipelineAST:
    """Abstract Syntax Tree for a Data-Juicer pipeline."""

    def __init__(self):
        self.root = None
        self._op_type_map = {
            'mapper': OpType.MAPPER,
            'filter': OpType.FILTER,
            'deduplicator': OpType.DEDUPLICATOR,
            'selector': OpType.SELECTOR,
            'grouper': OpType.GROUPER,
            'aggregator': OpType.AGGREGATOR
        }

        # Operation dependencies and optimization rules
        self._op_dependencies = {
            OpType.FILTER: {OpType.MAPPER},  # Filters can depend on mappers
            OpType.DEDUPLICATOR:
            {OpType.MAPPER,
             OpType.FILTER},  # Deduplicators can depend on mappers and filters
            OpType.SELECTOR:
            {OpType.MAPPER, OpType.FILTER,
             OpType.DEDUPLICATOR},  # Selectors can depend on all previous ops
            OpType.GROUPER: {
                OpType.MAPPER, OpType.FILTER, OpType.DEDUPLICATOR,
                OpType.SELECTOR
            },  # Groupers can depend on all previous ops
            OpType.AGGREGATOR:
            {OpType.GROUPER}  # Aggregators can only depend on groupers
        }

    def _get_op_type(self, op_name: str) -> OpType:
        """Determine the operation type from its name."""
        for suffix, op_type in self._op_type_map.items():
            if op_name.endswith(f'_{suffix}'):
                return op_type
        return OpType.MAPPER  # Default to mapper if type cannot be determined

    def build_from_config(self, config: Dict[str, Any]) -> None:
        """Build the AST from a configuration dictionary."""
        if 'process' not in config:
            raise ValueError("Configuration must contain a 'process' field")

        process_list = config['process']
        if not process_list:
            return

        # Create root node
        self.root = OpNode(
            name='root',
            op_type=OpType.ROOT,  # Root is a special type
            config={})

        # Build tree following the order in process_list
        current_node = self.root
        for op_config in process_list:
            op_name, op_args = list(op_config.items())[0]
            op_type = self._get_op_type(op_name)

            new_node = OpNode(name=op_name, op_type=op_type, config=op_args)
            current_node.add_child(new_node)
            current_node = new_node

    def build_from_yaml(self, yaml_path: str) -> None:
        """Build the AST from a YAML configuration file."""
        with open(yaml_path, 'r') as f:
            config = yaml.safe_load(f)
        self.build_from_config(config)

    def to_dict(self) -> Dict[str, Any]:
        """Convert the AST to a dictionary representation."""
        if not self.root:
            return {}
        return self.root.to_dict()

    def visualize(self) -> str:
        """Generate a string representation of the AST for visualization."""
        if not self.root:
            return 'Empty pipeline'

        def _visualize_node(node: OpNode,
                            level: int = 0,
                            is_last: bool = True) -> str:
            indent = '  ' * level
            prefix = '└── ' if is_last else '├── '
            result = f'{indent}{prefix}{node.name} ({node.op_type.value})\n'

            for i, child in enumerate(node.children):
                is_last_child = i == len(node.children) - 1
                result += _visualize_node(child, level + 1, is_last_child)
            return result

        return 'Pipeline:\n' + _visualize_node(self.root, 0, True)

    @staticmethod
    def is_mapper_op(node_or_type) -> bool:
        """Check if node or op_type is a mapper operation using value comparison."""
        if hasattr(node_or_type, 'op_type'):
            return getattr(node_or_type, 'op_type').value == 'mapper'
        return node_or_type.value == 'mapper'

    @staticmethod
    def is_filter_op(node_or_type) -> bool:
        """Check if node or op_type is a filter operation using value comparison."""
        if hasattr(node_or_type, 'op_type'):
            return getattr(node_or_type, 'op_type').value == 'filter'
        return node_or_type.value == 'filter'

    @staticmethod
    def op_type_equals(a, b) -> bool:
        """Compare OpType values safely to handle module import issues."""
        return (getattr(a, 'value', a) == getattr(b, 'value', b))


if __name__ == '__main__':
    import os

    # Set up argument parser
    parser = argparse.ArgumentParser(
        description='Build and visualize pipeline AST from config file')
    parser.add_argument(
        '--config',
        type=str,
        default='configs/data_juicer_recipes/pile-philpaper-refine.yaml',
        help='Path to the pipeline configuration file (YAML)')
    parser.add_argument(
        '--probe-results',
        type=str,
        help='Path to probe results file (YAML) containing operation speeds')
    parser.add_argument('--optimize',
                        action='store_true',
                        help='Apply optimization strategies to the pipeline')

    args = parser.parse_args()

    # Get absolute path to config file
    config_path = os.path.abspath(args.config)
    print(f'Using config file: {config_path}')

    # Load and process config
    config = yaml.safe_load(open(config_path, 'r'))

    # Build initial AST
    ast = PipelineAST()
    ast.build_from_config(config)
    print('\nOriginal Pipeline:')
    print(ast.visualize())

    # Apply optimization if requested
    if args.optimize:
        from data_juicer.core.optimizer.filter_fusion_strategy import \
            FilterFusionStrategy
        from data_juicer.core.optimizer.mapper_fusion_strategy import \
            MapperFusionStrategy
        from data_juicer.core.optimizer.optimizer import PipelineOptimizer

        # Load probe results if provided
        probe_results = None
        if args.probe_results:
            probe_path = os.path.abspath(args.probe_results)
            print(f'\nUsing probe results from: {probe_path}')
            probe_results = yaml.safe_load(open(probe_path, 'r'))

        # Create optimizer with filter fusion strategy
        optimizer = PipelineOptimizer([
            FilterFusionStrategy(probe_results=probe_results),
            MapperFusionStrategy()
        ])

        # Apply optimization
        optimized_ast = optimizer.optimize(ast)

        print('\nOptimized Pipeline:')
        print(optimized_ast.visualize())
