<!doctype html>
<html class="no-js" lang="zh-CN" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="ç´¢å¼•" href="../genindex.html" /><link rel="search" title="æœç´¢" href="../search.html" /><link rel="next" title="æ¼”ç¤º" href="../demos/README_ZH.html" /><link rel="prev" title="æ²™ç›’å®éªŒå®¤" href="Sandbox_ZH.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Awesome Data-Model Co-Development of MLLMs - data-juicer</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/sidebar-menu.css?v=dcc41d8a" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    


    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index_ZH.html"><div class="brand">data-juicer</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index_ZH.html">
  
  
  <span class="sidebar-brand-text">data-juicer</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="æœç´¢" name="q" aria-label="æœç´¢">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">æ•™ç¨‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorial/DJ-Cookbook_ZH.html">DJ-Cookbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial/Installation_ZH.html">å®‰è£…</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial/QuickStart_ZH.html">å¿«é€Ÿä¸Šæ‰‹</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">å¸®åŠ©æ–‡æ¡£</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Operators.html">Operator Schemas ç®—å­æè¦</a></li>
<li class="toctree-l1"><a class="reference internal" href="RecipeGallery_ZH.html">æ•°æ®èœè°±Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="DatasetCfg_ZH.html">æ•°æ®é›†é…ç½®æŒ‡å—</a></li>
<li class="toctree-l1"><a class="reference internal" href="BadDataExhibition_ZH.html">â€œåâ€æ•°æ®å±•è§ˆ</a></li>
<li class="toctree-l1"><a class="reference internal" href="DJ_SORA_ZH.html">DJ-SORA</a></li>
<li class="toctree-l1"><a class="reference internal" href="DJ_service_ZH.html">APIæœåŠ¡åŒ–</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeveloperGuide_ZH.html">å¼€å‘è€…æŒ‡å—</a></li>
<li class="toctree-l1"><a class="reference internal" href="Distributed_ZH.html">Data-Juicer åˆ†å¸ƒå¼æ•°æ®å¤„ç†</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sandbox_ZH.html">æ²™ç›’å®éªŒå®¤</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Awesome Data-Model Co-Development of MLLMs </a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">demos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../demos/README_ZH.html">æ¼”ç¤º</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/auto_evaluation_helm/README_ZH.html">è‡ªåŠ¨åŒ–è¯„æµ‹ï¼šHELM è¯„æµ‹åŠå¯è§†åŒ–</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos/role_playing_system_prompt/README_ZH.html">ä¸ºLLMæ„é€ è§’è‰²æ‰®æ¼”çš„system prompt</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">å·¥å…·</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tools/distributed_deduplication/README_ZH.html">åˆ†å¸ƒå¼æ¨¡ç³Šå»é‡å·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/evaluator/README_ZH.html">Auto Evaluation Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/evaluator/gpt_eval/README_ZH.html">GPT EVALï¼šä½¿ç”¨ OpenAI API è¯„æµ‹å¤§æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/evaluator/recorder/README_ZH.html">Evaluation Results Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/fmt_conversion/README_ZH.html">æ ¼å¼è½¬æ¢å·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/fmt_conversion/multimodal/README_ZH.html">å¤šæ¨¡æ€å·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/fmt_conversion/post_tuning_dialog/README_ZH.html">åå¾®è°ƒå·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/hpo/README_ZH.html">æ•°æ®èœè°±çš„è‡ªåŠ¨åŒ–è¶…å‚ä¼˜åŒ–</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/humanops/README.html">Label Studio Service Utility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/mm_eval/inception_metrics/README_ZH.html">è§†é¢‘ç”Ÿæˆæµ‹è¯„å·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/postprocess/README_ZH.html">Postprocess tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/preprocess/README_ZH.html">é¢„å¤„ç†å·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/quality_classifier/README_ZH.html">ç»™æ•°æ®æ‰“åˆ†</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ç¬¬ä¸‰æ–¹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../thirdparty/LLM_ecosystems/README_ZH.html">å¤§è¯­è¨€æ¨¡å‹ç”Ÿæ€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../thirdparty/models/README_ZH.html">ç¬¬ä¸‰æ–¹æ¨¡å‹åº“</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api.html">API Reference</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of API Reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data_juicer.core.html">data_juicer.core package</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of data_juicer.core package</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.core.data.html">data_juicer.core.data package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.core.executor.html">data_juicer.core.executor package</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data_juicer.ops.html">data_juicer.ops package</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of data_juicer.ops package</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.aggregator.html">data_juicer.ops.aggregator package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.common.html">data_juicer.ops.common package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.deduplicator.html">data_juicer.ops.deduplicator package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.filter.html">data_juicer.ops.filter package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.grouper.html">data_juicer.ops.grouper package</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../data_juicer.ops.mapper.html">data_juicer.ops.mapper package</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of data_juicer.ops.mapper package</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../data_juicer.ops.mapper.annotation.html">data_juicer.ops.mapper.annotation package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.selector.html">data_juicer.ops.selector package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.ops.filter.html">data_juicer.ops.filter package</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data_juicer.ops.mapper.html">data_juicer.ops.mapper package</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of data_juicer.ops.mapper package</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../data_juicer.ops.mapper.annotation.html">data_juicer.ops.mapper.annotation package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.ops.deduplicator.html">data_juicer.ops.deduplicator package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.ops.selector.html">data_juicer.ops.selector package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.ops.common.html">data_juicer.ops.common package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.analysis.html">data_juicer.analysis package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.config.html">data_juicer.config package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_juicer.format.html">data_juicer.format package</a></li>
</ul>
</li>
</ul>

</div></div><div class="sidebar-bottom-menu">
    <div class="current-info">
        <span class="current-lang">zh-CN</span>|<span class="current-version">main</span>
    </div>
    <div class="dropdown-panel">
        
        <div class="section">
            <dt>Language</dt>
            <dd>
                
                <a href="../../../en/main/docs/awesome_llm_data.html"
                    >
                    English
                </a>
                
                <a href="../../../zh_CN/main/docs/awesome_llm_data.html"
                    >
                    ç®€ä½“ä¸­æ–‡
                </a>
                
            </dd>
        </div>
        

        
        <div class="section">
            <dt>Version</dt>
            <dd>
                    <a href="awesome_llm_data.html">main</a>
            </dd>
        </div>
        
    </div>
</div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/docs/awesome_llm_data.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="awesome-data-model-co-development-of-mllms">
<h1>Awesome Data-Model Co-Development of MLLMs <a class="reference external" href="https://awesome.re"><img alt="Awesome" src="https://awesome.re/badge.svg" /></a><a class="headerlink" href="#awesome-data-model-co-development-of-mllms" title="Link to this heading">Â¶</a></h1>
<p>Welcome to the &quot;Awesome List&quot; for data-model co-development of Multi-Modal Large Language Models (MLLMs), a continually updated resource tailored for the open-source community. This compilation features cutting-edge research, insightful articles focusing on improving MLLMs involving with the data-model co-development of MLLMs, and  tagged based on the proposed <strong>taxonomy</strong> from our data-model co-development <a class="reference external" href="https://arxiv.org/abs/2407.08583">survey</a>, as illustrated below.</p>
<p><img alt="Overview of Our Taxonomy" src="https://img.alicdn.com/imgextra/i1/O1CN01aN3TVo1mgGZAuSHJ4_!!6000000004983-2-tps-3255-1327.png" />
Due to the rapid development in the field, this repository and our paper are continuously being updated and synchronized with each other. <strong>Please feel free to make pull requests or open issues to <a class="reference internal" href="#contribution-to-this-survey">contribute to</a> this list and add more related resources!</strong> We will periodically update our <a class="reference external" href="https://arxiv.org/abs/2407.08583">arXiv version</a> according to this repository.</p>
<section id="news">
<h2>News<a class="headerlink" href="#news" title="Link to this heading">Â¶</a></h2>
<ul class="simple">
<li><p>ğŸ‰ [2025-06-04] Our <a class="reference external" href="https://github.com/modelscope/data-juicer/blob/main/docs/hhttps:/ieeexplore.ieee.org/document/11027559">Data-Model Co-development Survey</a> has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)! Welcome to explore and contribute this awesome-list.</p></li>
<li><p>[2025-05-25] We added 20 academic papers related to this survey.</p></li>
<li><p><img alt="new" src="https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png" /> [2024-10-23] We built a <a class="reference external" href="https://modelscope.github.io/data-juicer/_static/awesome-list.html">dynamic table</a> based on the <a class="reference internal" href="#paper-list">paper list</a> that supports filtering and searching.</p></li>
<li><p><img alt="new" src="https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png" /> [2024-10-22] We restructured our <a class="reference internal" href="#paper-list">paper list</a> to provide more streamlined information.</p></li>
</ul>
</section>
<section id="candidate-co-development-tags">
<h2>Candidate Co-Development Tags<a class="headerlink" href="#candidate-co-development-tags" title="Link to this heading">Â¶</a></h2>
<p>These tags correspond to the taxonomy in our paper, and each work may be assigned with more than one tags.</p>
<section id="data4model-scaling">
<h3>Data4Model: Scaling<a class="headerlink" href="#data4model-scaling" title="Link to this heading">Â¶</a></h3>
<section id="for-scaling-up-of-mllms-larger-datasets">
<h4>For Scaling Up of MLLMs: Larger Datasets<a class="headerlink" href="#for-scaling-up-of-mllms-larger-datasets" title="Link to this heading">Â¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data Acquisition</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Data Augmentation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Data Diversity</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="for-scaling-effectiveness-of-mllms-better-subsets">
<h4>For Scaling Effectiveness of MLLMs: Better Subsets<a class="headerlink" href="#for-scaling-effectiveness-of-mllms-better-subsets" title="Link to this heading">Â¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data Condensation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Data Mixture</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Data Packing</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Cross-Modal Alignment</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="data4model-usability">
<h3>Data4Model: Usability<a class="headerlink" href="#data4model-usability" title="Link to this heading">Â¶</a></h3>
<section id="for-instruction-responsiveness-of-mllms">
<h4>For Instruction Responsiveness of MLLMs<a class="headerlink" href="#for-instruction-responsiveness-of-mllms" title="Link to this heading">Â¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Prompt Design</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>ICL Data</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Human-Behavior Alignment Data</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="for-reasoning-ability-of-mllms">
<h4>For Reasoning Ability of MLLMs<a class="headerlink" href="#for-reasoning-ability-of-mllms" title="Link to this heading">Â¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data for Single-Hop Reasoning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--SingleHop-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Data for Multi-Hop Reasoning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="for-ethics-of-mllms">
<h4>For Ethics of MLLMs<a class="headerlink" href="#for-ethics-of-mllms" title="Link to this heading">Â¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data Toxicity</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Data Privacy and Intellectual Property</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Privacy&amp;IP-d3f0aa" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="for-evaluation-of-mllms">
<h4>For Evaluation of MLLMs<a class="headerlink" href="#for-evaluation-of-mllms" title="Link to this heading">Â¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Benchmarks for Multi-Modal Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Benchmarks for Multi-Modal Generation:</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Benchmarks for Multi-Modal Retrieval:</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Retrieval-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Benchmarks for Multi-Modal Reasoning:</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="model4data-synthesis">
<h3>Model4Data: Synthesis<a class="headerlink" href="#model4data-synthesis" title="Link to this heading">Â¶</a></h3>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Model as a Data Creator</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Model as a Data Mapper</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Model as a Data Filter</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Model as a Data Evaluator</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="model4data-insights">
<h3>Model4Data: Insights<a class="headerlink" href="#model4data-insights" title="Link to this heading">Â¶</a></h3>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section Title</p></th>
<th class="head"><p>Tag</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Model as a Data Navigator</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Navigator-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>Model as a Data Extractor</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Extractor-f2c0c6" /></p></td>
</tr>
<tr class="row-even"><td><p>Model as a Data Analyzer</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>Model as a Data Visualizer</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Visualizer-f2c0c6" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="paper-list">
<h2>Paper List<a class="headerlink" href="#paper-list" title="Link to this heading">Â¶</a></h2>
<p>Below is a paper list summarized based on our survey. Additionally, we have provided a <a class="reference external" href="https://modelscope.github.io/data-juicer/_static/awesome-list.html">dynamic table</a> that supports filtering and searching, with the data source same as the list below.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Title</p></th>
<th class="head"><p>Tags</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Probing Heterogeneous Pretraining Datasets with Small Curated Datasets</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>ChartLlama: A Multimodal LLM for Chart Understanding and Generation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Visualizer-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>VideoChat: Chat-Centric Video Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Audio Retrieval with WavText5K and CLAP Training</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Retrieval-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Demystifying CLIP Data</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Learning Transferable Visual Models From Natural Language Supervision</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>DataComp: In search of the next generation of multimodal datasets</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Beyond neural scaling laws: beating power law scaling via data pruning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Flamingo: a visual language model for few-shot learning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Quality not quantity: On the interaction between dataset design and robustness of clip</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>VBench: Comprehensive Benchmark Suite for Video Generative Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>EvalCraftr: Benchmarking and Evaluating Large Video Generation Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Training Compute-Optimal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>NExT-GPT: Any-to-Any Multimodal LLM</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>ChartReformer: Natural Language-Driven Chart Image Editing</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Visualizer-f2c0c6" /></p></td>
</tr>
<tr class="row-even"><td><p>GroundingGPT: Language Enhanced Multi-modal Grounding Model</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Kosmos-2: Grounding Multimodal Large Language Models to the World</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>3DBench: A Scalable 3D Benchmark and Instruction-Tuning Dataset</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Structured Packing in LLM Training Improves Long Context Utilization</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>MoDE: CLIP Data Experts via Clustering</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Efficient Multimodal Learning from Data-centric Perspective</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Perception Test: A Diagnostic Benchmark for Multimodal Video Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>FunQA: Towards Surprising Video ComprehensionFunQA: Towards Surprising Video Comprehension</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>OneChart: Purify the Chart Structural Extraction via One Auxiliary Token</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--SingleHop-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>WorldGPT: Empowering LLM as Multimodal World Model</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Navigator-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Fewer Truncations Improve Language Modeling</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>UNIAA: A Unified Multi-modal Image Aesthetic Data AugmentationAssessment Baseline and Benchmark</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>M3DBench: Letâ€™s Instruct Large Models with Multi-modal 3D Prompts</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>MoqaGPT: Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /></p></td>
</tr>
<tr class="row-even"><td><p>mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Retrieval-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>On the Adversarial Robustness of Multi-Modal Foundation Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--SingleHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>PaLM-E: An Embodied Multimodal Language Model</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Multimodal Data Curation via Object Detection and Filter Ensembles</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Sieve: Multimodal Dataset Pruning Using Image Captioning Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Towards a statistical theory of data selection under weak supervision</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>ğ·2 Pruning: Message Passing for Balancing Diversity &amp; Difficulty in Data Pruning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>UIClip: A Data-driven Model for Assessing User Interface Design</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>CapsFusion: Rethinking Image-Text Data at Scale</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Improving CLIP Training with Language Rewrites</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>A Decade's Battle on Dataset Bias: Are We There Yet?</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Data Filtering Networks</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Align and Attend: Multimodal Summarization with Dual Contrastive Losses</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--SingleHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Text-centric Alignment for Multi-Modality Learning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Noisy Correspondence Learning with Meta Similarity Correction</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Language-Image Models with 3D Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--SingleHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Scaling Laws for Generative Mixed-Modal Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>BLINK: Multimodal Large Language Models Can See but Not Perceive</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Visual Hallucinations of Multi-modal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Visual Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>On the De-duplication of LAION-2B</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Data Augmentation for Text-based Person Retrieval Using Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Aligning Actions and Walking to LLM-Generated Textual Descriptions</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Probing Multimodal LLMs as World Models for Driving</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Unified Hallucination Detection for Multimodal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Extractor-f2c0c6" /><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Semdedup: Data-efficient learning at web-scale through semantic deduplication</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Automated Multi-level Preference for MLLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Silkie: Preference distillation for large visual language models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>M3it: A large-scale dataset towards multi-modal multilingual instruction tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Aligning Large Multimodal Models with Factually Augmented RLHF</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Retrieval-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>ImgTrojan: Jailbreaking Vision-Language Models with ONE Image</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Improving Multimodal Datasets with Image Captioning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /></p></td>
</tr>
<tr class="row-even"><td><p>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Extractor-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Extractor-f2c0c6" /><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>CiT: Curation in Training for Effective Vision-Language Data</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>InstructPix2Pix: Learning to Follow Image Editing Instructions</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Visualizer-f2c0c6" /></p></td>
</tr>
<tr class="row-odd"><td><p>ModelGo: A Practical Tool for Machine Learning License Analysis</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Privacy&amp;IP-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>Scaling Laws of Synthetic Images for Model Training ... for Now</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Segment Anything</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p></td>
</tr>
<tr class="row-even"><td><p>All in an Aggregated Image for In-Image Learning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p></td>
</tr>
<tr class="row-odd"><td><p>Panda-70m: Captioning 70m videos with multiple cross-modality teachers</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Imagebind: One embedding space to bind them all</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-even"><td><p>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p></td>
</tr>
<tr class="row-odd"><td><p>Genixer: Empowering Multimodal Large Language Model as a Powerful Data Generator</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-even"><td><p>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /> <img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p></td>
</tr>
<tr class="row-odd"><td><p>Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation</p></td>
<td><p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="contribution-to-this-survey">
<h2>Contribution to This Survey<a class="headerlink" href="#contribution-to-this-survey" title="Link to this heading">Â¶</a></h2>
<p>Due to the rapid development in the field, this repository and our paper are continuously being updated and synchronized with each other. Please feel free to make pull requests or open issues to contribute to this list and add more related resources!
<strong>You can add the titles of relevant papers to the table above, and (optionally) provide suggested tags along with the corresponding sections if possible.</strong>
We will attempt to complete the remaining information and periodically update our survey based on the updated content of this document.</p>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Link to this heading">Â¶</a></h3>
<p>If you find our work useful for your research or development, please kindly cite the following <a class="reference external" href="https://arxiv.org/abs/2407.08583">paper</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@article</span><span class="p">{</span><span class="n">qin2024synergy</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">The</span> <span class="n">Synergy</span> <span class="n">between</span> <span class="n">Data</span> <span class="ow">and</span> <span class="n">Multi</span><span class="o">-</span><span class="n">Modal</span> <span class="n">Large</span> <span class="n">Language</span> <span class="n">Models</span><span class="p">:</span> <span class="n">A</span> <span class="n">Survey</span> <span class="kn">from</span><span class="w"> </span><span class="nn">Co</span><span class="o">-</span><span class="n">Development</span> <span class="n">Perspective</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Qin</span><span class="p">,</span> <span class="n">Zhen</span> <span class="ow">and</span> <span class="n">Chen</span><span class="p">,</span> <span class="n">Daoyuan</span> <span class="ow">and</span> <span class="n">Zhang</span><span class="p">,</span> <span class="n">Wenhao</span> <span class="ow">and</span> <span class="n">Liuyi</span><span class="p">,</span> <span class="n">Yao</span> <span class="ow">and</span> <span class="n">Yilun</span><span class="p">,</span> <span class="n">Huang</span> <span class="ow">and</span> <span class="n">Ding</span><span class="p">,</span> <span class="n">Bolin</span> <span class="ow">and</span> <span class="n">Li</span><span class="p">,</span> <span class="n">Yaliang</span> <span class="ow">and</span> <span class="n">Deng</span><span class="p">,</span> <span class="n">Shuiguang</span><span class="p">},</span>
  <span class="n">journal</span><span class="o">=</span><span class="p">{</span><span class="n">arXiv</span> <span class="n">preprint</span> <span class="n">arXiv</span><span class="p">:</span><span class="mf">2407.08583</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2024</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="section-mentioned-papers-retrieval-list">
<h2>&quot;Section - Mentioned Papers&quot; Retrieval List<a class="headerlink" href="#section-mentioned-papers-retrieval-list" title="Link to this heading">Â¶</a></h2>
<p>We provide a collapsible list of back reference, allowing readers to see which (sub)section mention the papers from the table above.
The collapsible list of back reference will be periodically updated based on the content of the table and our paper.</p>
<details>
<summary>Sec. 3.1  For Scaling of MLLMs: Larger Datasets</summary>
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>Training Compute-Optimal Large Language Models</p></li>
</ul>
<details>
<summary>Sec. 3.1.1  Data Acquisition</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Acquisition-f1db9d" /></p>
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting</p></li>
<li><p>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</p></li>
<li><p>Audio Retrieval with WavText5K and CLAP Training</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>Learning Transferable Visual Models From Natural Language Supervision</p></li>
<li><p>NExT-GPT: Any-to-Any Multimodal LLM</p></li>
<li><p>ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization</p></li>
<li><p>ChartReformer: Natural Language-Driven Chart Image Editing</p></li>
<li><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></li>
<li><p>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</p></li>
<li><p>StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding</p></li>
<li><p>MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</p></li>
<li><p>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs</p></li>
<li><p>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</p></li>
<li><p>ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction</p></li>
<li><p>TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models</p></li>
<li><p>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</p></li>
<li><p>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</p></li>
<li><p>UIClip: A Data-driven Model for Assessing User Interface Design</p></li>
<li><p>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</p></li>
<li><p>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</p></li>
<li><p>Visual Instruction Tuning</p></li>
<li><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></li>
<li><p>Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</p></li>
<li><p>Probing Multimodal LLMs as World Models for Driving</p></li>
<li><p>Genixer: Empowering Multimodal Large Language Model as a Powerful Data Generator</p></li>
<li><p>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</p></li>
<li><p>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</p></li>
<li><p>Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception</p></li>
<li><p>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</p></li>
<li><p>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</p></li>
</ul>
</details>
<details>
<summary>Sec. 3.1.2  Data Augmentation</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Augmentation-f1db9d" /></p>
<ul class="simple">
<li><p>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</p></li>
<li><p>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration</p></li>
<li><p>Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives</p></li>
<li><p>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</p></li>
<li><p>CapsFusion: Rethinking Image-Text Data at Scale</p></li>
<li><p>Improving CLIP Training with Language Rewrites</p></li>
<li><p>Data Augmentation for Text-based Person Retrieval Using Large Language Models</p></li>
<li><p>Aligning Actions and Walking to LLM-Generated Textual Descriptions</p></li>
<li><p>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</p></li>
</ul>
</details>
<details>
<summary>Sec. 3.1.3  Data Diversity</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Up--Diversity-f1db9d" /></p>
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>Audio Retrieval with WavText5K and CLAP Training</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>Flamingo: a visual language model for few-shot learning</p></li>
<li><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></li>
<li><p>ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</p></li>
<li><p>Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models</p></li>
<li><p>PaLM-E: An Embodied Multimodal Language Model</p></li>
<li><p>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</p></li>
<li><p>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</p></li>
<li><p>MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct</p></li>
</ul>
</details>
</details>
<details>
<summary>Sec. 3.2  For Scaling Effectiveness of MLLMs: Better Subsets</summary>
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
</ul>
<details>
<summary>Sec. 3.2.1  Data Condensation</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Condensation-f1db9d" /></p>
<ul class="simple">
<li><p>The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>Beyond neural scaling laws: beating power law scaling via data pruning</p></li>
<li><p>Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters</p></li>
<li><p>Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training</p></li>
<li><p>Efficient Multimodal Learning from Data-centric Perspective</p></li>
<li><p>Multimodal Data Curation via Object Detection and Filter Ensembles</p></li>
<li><p>Sieve: Multimodal Dataset Pruning Using Image Captioning Models</p></li>
<li><p>Towards a statistical theory of data selection under weak supervision</p></li>
<li><p>Data Filtering Networks</p></li>
<li><p>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</p></li>
<li><p>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</p></li>
<li><p>Semdedup: Data-efficient learning at web-scale through semantic deduplication</p></li>
<li><p>On the De-duplication of LAION-2B</p></li>
<li><p>Improving Multimodal Datasets with Image Captioning</p></li>
<li><p>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</p></li>
</ul>
</details>
<details>
<summary>Sec. 3.2.2  Data Mixture</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Mixture-f1db9d" /></p>
<ul class="simple">
<li><p>Learning Transferable Visual Models From Natural Language Supervision</p></li>
<li><p>Flamingo: a visual language model for few-shot learning</p></li>
<li><p>Quality not quantity: On the interaction between dataset design and robustness of clip</p></li>
<li><p>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs</p></li>
<li><p>A Decade's Battle on Dataset Bias: Are We There Yet?</p></li>
<li><p>Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</p></li>
<li><p>Demystifying CLIP Data</p></li>
</ul>
</details>
<details>
<summary>Sec. 3.2.3  Data Packing</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--Packing-f1db9d" /></p>
<ul class="simple">
<li><p>Structured Packing in LLM Training Improves Long Context Utilization</p></li>
<li><p>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</p></li>
<li><p>MoDE: CLIP Data Experts via Clustering</p></li>
<li><p>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</p></li>
<li><p>Fewer Truncations Improve Language Modeling</p></li>
</ul>
</details>
<details>
<summary>Sec. 3.2.4 Cross-Modal Alignment</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Scaling--Effectiveness--CrossModalAlignment-f1db9d" /></p>
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>Multimodal Data Curation via Object Detection and Filter Ensembles</p></li>
<li><p>Sieve: Multimodal Dataset Pruning Using Image Captioning Models</p></li>
<li><p>ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization</p></li>
<li><p>Data Filtering Networks</p></li>
<li><p>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</p></li>
<li><p>Text-centric Alignment for Multi-Modality Learning</p></li>
<li><p>Noisy Correspondence Learning with Meta Similarity Correction</p></li>
<li><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></li>
<li><p>Semdedup: Data-efficient learning at web-scale through semantic deduplication</p></li>
<li><p>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</p></li>
<li><p>AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability</p></li>
<li><p>Improving Multimodal Datasets with Image Captioning</p></li>
<li><p>Imagebind: One embedding space to bind them all</p></li>
<li><p>UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</p></li>
<li><p>FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion</p></li>
<li><p>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</p></li>
<li><p>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</p></li>
</ul>
</details>
</details>
<details>
<summary>Sec. 4.1  For Instruction Responsiveness of MLLMs</summary>
<ul class="simple">
<li><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></li>
</ul>
<details>
<summary>Sec. 4.1.1  Prompt Design</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--Prompt-d3f0aa" /></p>
<ul class="simple">
<li><p>Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic</p></li>
<li><p>Kosmos-2: Grounding Multimodal Large Language Models to the World</p></li>
<li><p>Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</p></li>
<li><p>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</p></li>
<li><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></li>
<li><p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models|</p></li>
<li><p>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</p></li>
<li><p>Scaling Laws of Synthetic Images for Model Training ... for Now</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.1.2  ICL Data</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--ICL-d3f0aa" /></p>
<ul class="simple">
<li><p>GroundingGPT: Language Enhanced Multi-modal Grounding Model</p></li>
<li><p>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs</p></li>
<li><p>Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models</p></li>
<li><p>All in an Aggregated Image for In-Image Learning</p></li>
<li><p>AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning</p></li>
<li><p>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.1.3  Human-Behavior Alignment Data</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Following--HumanBehavior-d3f0aa" /></p>
<ul class="simple">
<li><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></li>
<li><p>MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria</p></li>
<li><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></li>
<li><p>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</p></li>
<li><p>Automated Multi-level Preference for MLLMs</p></li>
<li><p>Silkie: Preference distillation for large visual language models</p></li>
<li><p>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</p></li>
<li><p>Aligning Large Multimodal Models with Factually Augmented RLHF</p></li>
<li><p>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</p></li>
<li><p>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</p></li>
</ul>
</details>
</details>
<details>
<summary>Sec. 4.2  For Reasoning Ability of MLLMs</summary>
<details>
<summary>Sec. 4.2.1  Data for Single-Hop Reasoning</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--SingleHop-d3f0aa" /></p>
<ul class="simple">
<li><p>FunQA: Towards Surprising Video ComprehensionFunQA: Towards Surprising Video Comprehension</p></li>
<li><p>StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding</p></li>
<li><p>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.2.2  Data for Multi-Hop Reasoning</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Reasoning--MultiHop-d3f0aa" /></p>
<ul class="simple">
<li><p>MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale</p></li>
<li><p>Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos</p></li>
<li><p>Language-Image Models with 3D Understanding</p></li>
<li><p>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</p></li>
<li><p>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</p></li>
<li><p>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</p></li>
<li><p>Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models</p></li>
</ul>
</details>
</details>
<details>
<summary>Sec. 4.3  For Ethics of MLLMs</summary>
<details>
<summary>Sec. 4.3.1  Data Toxicity</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Toxicity-d3f0aa" /></p>
<ul class="simple">
<li><p>Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain</p></li>
<li><p>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</p></li>
<li><p>The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative</p></li>
<li><p>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</p></li>
<li><p>ImgTrojan: Jailbreaking Vision-Language Models with ONE Image</p></li>
<li><p>VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models</p></li>
<li><p>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</p></li>
<li><p>On the Adversarial Robustness of Multi-Modal Foundation Models</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.3.2  Data Privacy and Intellectual Property</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Ethic--Privacy&amp;IP-d3f0aa" /></p>
<ul class="simple">
<li><p>ModelGo: A Practical Tool for Machine Learning License Analysis</p></li>
</ul>
</details>
</details>
<details>
<summary>Sec. 4.4 For Evaluation of MLLMs</summary>
<details>
<summary>Sec. 4.4.1  Benchmarks for Multi-Modal Understanding</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Understanding-d3f0aa" /></p>
<ul class="simple">
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>3DBench: A Scalable 3D Benchmark and Instruction-Tuning Dataset</p></li>
<li><p>MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</p></li>
<li><p>SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</p></li>
<li><p>OneChart: Purify the Chart Structural Extraction via One Auxiliary Token</p></li>
<li><p>MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</p></li>
<li><p>ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction</p></li>
<li><p>UNIAA: A Unified Multi-modal Image Aesthetic Data AugmentationAssessment Baseline and Benchmark</p></li>
<li><p>M3DBench: Letâ€™s Instruct Large Models with Multi-modal 3D Prompts</p></li>
<li><p>Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation</p></li>
<li><p>BLINK: Multimodal Large Language Models Can See but Not Perceive</p></li>
<li><p>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.4.2 Benchmarks for Multi-Modal Generation</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Generation-d3f0aa" /></p>
<ul class="simple">
<li><p>VBench: Comprehensive Benchmark Suite for Video Generative Models</p></li>
<li><p>EvalCraftr: Benchmarking and Evaluating Large Video Generation Models</p></li>
<li><p>Perception Test: A Diagnostic Benchmark for Multimodal Video Models</p></li>
<li><p>WorldGPT: Empowering LLM as Multimodal World Model</p></li>
<li><p>MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria</p></li>
<li><p>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</p></li>
<li><p>OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation</p></li>
<li><p>Visual Hallucinations of Multi-modal Large Language Models</p></li>
<li><p>Unified Hallucination Detection for Multimodal Large Language Models</p></li>
<li><p>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.4.3  Benchmarks for Multi-Modal Retrieval</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Retrieval-d3f0aa" /></p>
<ul class="simple">
<li><p>Audio Retrieval with WavText5K and CLAP Training</p></li>
<li><p>MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI</p></li>
<li><p>Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation</p></li>
</ul>
</details>
<details>
<summary>Sec. 4.4.4  Benchmarks for Multi-Modal Reasoning</summary>
<p><img alt="" src="https://img.shields.io/badge/Data4Model--Usability--Eval--Reasoning-d3f0aa" /></p>
<ul class="simple">
<li><p>FunQA: Towards Surprising Video ComprehensionFunQA: Towards Surprising Video Comprehension</p></li>
<li><p>ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning</p></li>
<li><p>ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</p></li>
<li><p>Probing Multimodal LLMs as World Models for Driving</p></li>
<li><p>M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</p></li>
</ul>
</details>
</details>
<details>
<summary>Sec. 5.1  Model as a Data Creator</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Creator-b4d4fb" /></p>
<ul class="simple">
<li><p>What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning</p></li>
<li><p>ChartLlama: A Multimodal LLM for Chart Understanding and Generation</p></li>
<li><p>VideoChat: Chat-Centric Video Understanding</p></li>
<li><p>3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding</p></li>
<li><p>Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters</p></li>
<li><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></li>
<li><p>OneChart: Purify the Chart Structural Extraction via One Auxiliary Token</p></li>
<li><p>ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</p></li>
<li><p>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</p></li>
<li><p>UNIAA: A Unified Multi-modal Image Aesthetic Data AugmentationAssessment Baseline and Benchmark</p></li>
<li><p>Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives</p></li>
<li><p>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</p></li>
<li><p>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</p></li>
<li><p>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</p></li>
<li><p>InstructPix2Pix: Learning to Follow Image Editing Instructions</p></li>
<li><p>Genixer: Empowering Multimodal Large Language Model as a Powerful Data Generator</p></li>
<li><p>World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering</p></li>
<li><p>Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation</p></li>
</ul>
</details>
<details>
<summary>Sec. 5.2 Model as a Data Mapper</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Mapper-b4d4fb" /></p>
<ul class="simple">
<li><p>VideoChat: Chat-Centric Video Understanding</p></li>
<li><p>Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex</p></li>
<li><p>GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting</p></li>
<li><p>MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale</p></li>
<li><p>AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception</p></li>
<li><p>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</p></li>
<li><p>MoqaGPT: Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model</p></li>
<li><p>Improving CLIP Training with Language Rewrites</p></li>
<li><p>Data Augmentation for Text-based Person Retrieval Using Large Language Models</p></li>
<li><p>Aligning Actions and Walking to LLM-Generated Textual Descriptions</p></li>
<li><p>Unified Hallucination Detection for Multimodal Large Language Models</p></li>
<li><p>PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs</p></li>
<li><p>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</p></li>
<li><p>MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct</p></li>
<li><p>Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs</p></li>
<li><p>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</p></li>
<li><p>Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception</p></li>
<li><p>REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset</p></li>
<li><p>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</p></li>
<li><p>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</p></li>
</ul>
</details>
<details>
<summary>Sec. 5.3  Model as a Data Filter</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Filter-b4d4fb" /></p>
<ul class="simple">
<li><p>The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering</p></li>
<li><p>Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</p></li>
<li><p>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</p></li>
<li><p>Towards a statistical theory of data selection under weak supervision</p></li>
<li><p>Visual Hallucinations of Multi-modal Large Language Models</p></li>
<li><p>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</p></li>
<li><p>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering</p></li>
</ul>
</details>
<details>
<summary>Sec. 5.4  Model as a Data Evaluator</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Synthesis--Evaluator-b4d4fb" /></p>
<ul class="simple">
<li><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></li>
<li><p>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</p></li>
<li><p>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</p></li>
<li><p>MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria</p></li>
<li><p>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</p></li>
<li><p>ImgTrojan: Jailbreaking Vision-Language Models with ONE Image</p></li>
<li><p>ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs</p></li>
<li><p>FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models</p></li>
<li><p>A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment</p></li>
</ul>
</details>
<details>
<summary>Sec. 6.1  Model as a Data Navigator</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Navigator-f2c0c6" /></p>
<ul class="simple">
<li><p>How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?</p></li>
</ul>
</details>
<details>
<summary>Sec. 6.2  Model as a Data Extractor</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Extractor-f2c0c6" /></p>
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</p></li>
<li><p>Unified Hallucination Detection for Multimodal Large Language Models</p></li>
<li><p>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</p></li>
</ul>
</details>
<details>
<summary>Sec. 6.3  Model as a Data Analyzer</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Analyzer-f2c0c6" /></p>
<ul class="simple">
<li><p>ChartLlama: A Multimodal LLM for Chart Understanding and Generation</p></li>
<li><p>OneChart: Purify the Chart Structural Extraction via One Auxiliary Token</p></li>
<li><p>ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning</p></li>
<li><p>StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding</p></li>
<li><p>ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</p></li>
<li><p>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</p></li>
<li><p>mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding</p></li>
<li><p>mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model</p></li>
<li><p>Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System</p></li>
</ul>
</details>
<details>
<summary>Sec. 6.4  Model as a Data Visualizer</summary>
<p><img alt="" src="https://img.shields.io/badge/Model4Data--Insights--Visualizer-f2c0c6" /></p>
<ul class="simple">
<li><p>ChartLlama: A Multimodal LLM for Chart Understanding and Generation</p></li>
<li><p>ChartReformer: Natural Language-Driven Chart Image Editing</p></li>
<li><p>Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study</p></li>
</ul>
</details>
<details>
<summary>Sec. 8.1  Data-Model Co-Development Infrastructures</summary>
<ul class="simple">
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
</ul>
</details>
<details>
<summary>Sec. 8.2  Externally-Boosted MLLM Development</summary>
<details>
<summary>Sec. 8.2.1  MLLM-Based Data Discovery</summary> 
<ul class="simple">
<li><p>No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</p></li>
<li><p>ModelGo: A Practical Tool for Machine Learning License Analysis</p></li>
</ul>
</details>
<details>
<summary>Sec. 8.2.2  Modality-Compatibility Detection with MLLMs</summary>
<ul class="simple">
<li><p>Improving Multimodal Datasets with Image Captioning</p></li>
</ul>
</details>
<details>
<summary>Sec. 8.2.3  Automatic Knowledge Transfer for MLLMs</summary> 
- Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation
- MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria
</details>
</details>
<details>
<summary>Sec. 8.3  Self-Boosted MLLM Development</summary>
<details>
<summary>Sec. 8.3.1  Self Data Scaling with MLLMs</summary> 
<ul class="simple">
<li><p>Sieve: Multimodal Dataset Pruning Using Image Captioning Models</p></li>
<li><p>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</p></li>
<li><p>Segment Anything</p></li>
</ul>
</details>
<details>
<summary>Sec. 8.3.2  Self Data Condensation with MLLMs</summary>
</details>
<details>
<summary>Sec. 8.3.3  RL from Self Feedback of MLLMs</summary>
<ul class="simple">
<li><p>The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering</p></li>
<li><p>DataComp: In search of the next generation of multimodal datasets</p></li>
<li><p>Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters</p></li>
<li><p>Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training</p></li>
<li><p>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</p></li>
<li><p>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</p></li>
<li><p>Multimodal Data Curation via Object Detection and Filter Ensembles</p></li>
<li><p>Sieve: Multimodal Dataset Pruning Using Image Captioning Models</p></li>
<li><p>Data Filtering Networks</p></li>
<li><p>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</p></li>
<li><p>Semdedup: Data-efficient learning at web-scale through semantic deduplication</p></li>
<li><p>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</p></li>
<li><p>CiT: Curation in Training for Effective Vision-Language Data</p></li>
<li><p>Improving Multimodal Datasets with Image Captioning</p></li>
</ul>
</details>
</details>
<details>
<summary>Tab. 2</summary>
- No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
- DataComp: In search of the next generation of multimodal datasets
- TextSquare: Scaling up Text-Centric Visual Instruction Tuning
- MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria
- Align and Attend: Multimodal Summarization with Dual Contrastive Losses
- MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
- BLINK: Multimodal Large Language Models Can See but Not Perceive
- Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering
- ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model
- LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark
- Unified Hallucination Detection for Multimodal Large Language Models
- Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning
- M3it: A large-scale dataset towards multi-modal multilingual instruction tuning
- MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
- MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI
- M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought
- Panda-70m: Captioning 70m videos with multiple cross-modality teachers
- Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text
- ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning
</details>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../demos/README_ZH.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">æ¼”ç¤º</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="Sandbox_ZH.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">æ²™ç›’å®éªŒå®¤</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, Data-Juicer Team
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Awesome Data-Model Co-Development of MLLMs </a><ul>
<li><a class="reference internal" href="#news">News</a></li>
<li><a class="reference internal" href="#candidate-co-development-tags">Candidate Co-Development Tags</a><ul>
<li><a class="reference internal" href="#data4model-scaling">Data4Model: Scaling</a><ul>
<li><a class="reference internal" href="#for-scaling-up-of-mllms-larger-datasets">For Scaling Up of MLLMs: Larger Datasets</a></li>
<li><a class="reference internal" href="#for-scaling-effectiveness-of-mllms-better-subsets">For Scaling Effectiveness of MLLMs: Better Subsets</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data4model-usability">Data4Model: Usability</a><ul>
<li><a class="reference internal" href="#for-instruction-responsiveness-of-mllms">For Instruction Responsiveness of MLLMs</a></li>
<li><a class="reference internal" href="#for-reasoning-ability-of-mllms">For Reasoning Ability of MLLMs</a></li>
<li><a class="reference internal" href="#for-ethics-of-mllms">For Ethics of MLLMs</a></li>
<li><a class="reference internal" href="#for-evaluation-of-mllms">For Evaluation of MLLMs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model4data-synthesis">Model4Data: Synthesis</a></li>
<li><a class="reference internal" href="#model4data-insights">Model4Data: Insights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#paper-list">Paper List</a></li>
<li><a class="reference internal" href="#contribution-to-this-survey">Contribution to This Survey</a><ul>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li><a class="reference internal" href="#section-mentioned-papers-retrieval-list">&quot;Section - Mentioned Papers&quot; Retrieval List</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div>
<script src="../_static/documentation_options.js?v=2491dde2"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    </body>
</html>