[{"index": 1, "class_name": "audio_add_gaussian_noise_mapper", "class_desc": "Mapper to add gaussian noise to audio.", "arguments": "        min_amplitude (<class 'float'>): \n        max_amplitude (<class 'float'>): \n        p (<class 'float'>): "}, {"index": 2, "class_name": "audio_ffmpeg_wrapped_mapper", "class_desc": "Simple wrapper for FFmpeg audio filters.", "arguments": "        filter_name (typing.Optional[str]): ffmpeg audio filter name.\n        filter_kwargs (typing.Optional[typing.Dict]): keyword-arguments passed to ffmpeg filter.\n        global_args (typing.Optional[typing.List[str]]): list-arguments passed to ffmpeg command-line.\n        capture_stderr (<class 'bool'>): whether to capture stderr.\n        overwrite_output (<class 'bool'>): whether to overwrite output file."}, {"index": 3, "class_name": "calibrate_qa_mapper", "class_desc": "Mapper to calibrate question-answer pairs based on reference text.", "arguments": "        api_model (<class 'str'>): API model name.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the calibration task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        reference_template (typing.Optional[str]): Template for formatting the reference text.\n        qa_pair_template (typing.Optional[str]): Template for formatting question-answer pairs.\n        output_pattern (typing.Optional[str]): Regular expression for parsing model output.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 4, "class_name": "calibrate_query_mapper", "class_desc": "Mapper to calibrate query in question-answer pairs based on reference text.", "arguments": "        api_model (<class 'str'>): API model name.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the calibration task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        reference_template (typing.Optional[str]): Template for formatting the reference text.\n        qa_pair_template (typing.Optional[str]): Template for formatting question-answer pairs.\n        output_pattern (typing.Optional[str]): Regular expression for parsing model output.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 5, "class_name": "calibrate_response_mapper", "class_desc": "Mapper to calibrate response in question-answer pairs based on reference text.", "arguments": "        api_model (<class 'str'>): API model name.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the calibration task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        reference_template (typing.Optional[str]): Template for formatting the reference text.\n        qa_pair_template (typing.Optional[str]): Template for formatting question-answer pairs.\n        output_pattern (typing.Optional[str]): Regular expression for parsing model output.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 6, "class_name": "chinese_convert_mapper", "class_desc": "Mapper to convert Chinese between Traditional Chinese, Simplified Chinese and Japanese Kanji.", "arguments": "        mode (<class 'str'>): Choose the mode to convert Chinese:  s2t: Simplified Chinese to Traditional Chinese,  t2s: Traditional Chinese to Simplified Chinese,  s2tw: Simplified Chinese to Traditional Chinese (Taiwan Standard),  tw2s: Traditional Chinese (Taiwan Standard) to Simplified Chinese,  s2hk: Simplified Chinese to Traditional Chinese (Hong Kong variant),  hk2s: Traditional Chinese (Hong Kong variant) to Simplified Chinese,  s2twp: Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom,  tw2sp: Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom,  t2tw: Traditional Chinese to Traditional Chinese (Taiwan Standard),  tw2t: Traditional Chinese (Taiwan standard) to Traditional Chinese,  hk2t: Traditional Chinese (Hong Kong variant) to Traditional Chinese,  t2hk: Traditional Chinese to Traditional Chinese (Hong Kong variant),  t2jp: Traditional Chinese Characters (Ky\u016bjitai) to New Japanese Kanji,  jp2t: New Japanese Kanji (Shinjitai) to Traditional Chinese Characters, "}, {"index": 7, "class_name": "clean_copyright_mapper", "class_desc": "Mapper to clean copyright comments at the beginning of the text samples.", "arguments": ""}, {"index": 8, "class_name": "clean_email_mapper", "class_desc": "Mapper to clean email in text samples.", "arguments": "        pattern (typing.Optional[str]): regular expression pattern to search for within text.\n        repl (<class 'str'>): replacement string, default is empty string."}, {"index": 9, "class_name": "clean_html_mapper", "class_desc": "Mapper to clean html code in text samples.", "arguments": ""}, {"index": 10, "class_name": "clean_ip_mapper", "class_desc": "Mapper to clean ipv4 and ipv6 address in text samples.", "arguments": "        pattern (typing.Optional[str]): regular expression pattern to search for within text.\n        repl (<class 'str'>): replacement string, default is empty string."}, {"index": 11, "class_name": "clean_links_mapper", "class_desc": "Mapper to clean links like http/https/ftp in text samples.", "arguments": "        pattern (typing.Optional[str]): regular expression pattern to search for within text.\n        repl (<class 'str'>): replacement string, default is empty string."}, {"index": 12, "class_name": "dialog_intent_detection_mapper", "class_desc": "Mapper to generate user's intent labels in dialog. Input from history_key, query_key and response_key. Output lists of labels and analysis for queries in the dialog.", "arguments": "        api_model (<class 'str'>): API model name.\n        intent_candidates (typing.Optional[typing.List[str]]): The output intent candidates. Use the intent labels of the open domain if it is None.\n        max_round (typing.Annotated[int, Ge(ge=0)]): The max num of round in the dialog to build the prompt.\n        labels_key (<class 'str'>): The key name in the meta field to store the output labels. It is 'dialog_intent_labels' in default.\n        analysis_key (<class 'str'>): The key name in the meta field to store the corresponding analysis. It is 'dialog_intent_labels_analysis' in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        query_template (typing.Optional[str]): Template for query part to build the input prompt.\n        response_template (typing.Optional[str]): Template for response part to build the input prompt.\n        candidate_template (typing.Optional[str]): Template for intent candidates to build the input prompt.\n        analysis_template (typing.Optional[str]): Template for analysis part to build the input prompt.\n        labels_template (typing.Optional[str]): Template for labels to build the input prompt.\n        analysis_pattern (typing.Optional[str]): Pattern to parse the return intent analysis.\n        labels_pattern (typing.Optional[str]): Pattern to parse the return intent labels.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 13, "class_name": "dialog_sentiment_detection_mapper", "class_desc": "Mapper to generate user's sentiment labels in dialog. Input from history_key, query_key and response_key. Output lists of labels and analysis for queries in the dialog.", "arguments": "        api_model (<class 'str'>): API model name.\n        sentiment_candidates (typing.Optional[typing.List[str]]): The output sentiment candidates. Use open-domain sentiment labels if it is None.\n        max_round (typing.Annotated[int, Ge(ge=0)]): The max num of round in the dialog to build the prompt.\n        labels_key (<class 'str'>): The key name in the meta field to store the output labels. It is 'dialog_sentiment_labels' in default.\n        analysis_key (<class 'str'>): The key name in the meta field to store the corresponding analysis. It is 'dialog_sentiment_labels_analysis' in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        query_template (typing.Optional[str]): Template for query part to build the input prompt.\n        response_template (typing.Optional[str]): Template for response part to build the input prompt.\n        candidate_template (typing.Optional[str]): Template for sentiment candidates to build the input prompt.\n        analysis_template (typing.Optional[str]): Template for analysis part to build the input prompt.\n        labels_template (typing.Optional[str]): Template for labels part to build the input prompt.\n        analysis_pattern (typing.Optional[str]): Pattern to parse the return sentiment analysis.\n        labels_pattern (typing.Optional[str]): Pattern to parse the return sentiment labels.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 14, "class_name": "dialog_sentiment_intensity_mapper", "class_desc": "Mapper to predict user's sentiment intensity (from -5 to 5 in default prompt) in dialog. Input from history_key, query_key and response_key. Output lists of intensities and analysis for queries in the dialog.", "arguments": "        api_model (<class 'str'>): API model name.\n        max_round (typing.Annotated[int, Ge(ge=0)]): The max num of round in the dialog to build the prompt.\n        intensities_key (<class 'str'>): The key name in the meta field to store the output sentiment intensities. It is 'dialog_sentiment_intensity' in default.\n        analysis_key (<class 'str'>): The key name in the meta field to store the corresponding analysis. It is 'dialog_sentiment_intensity_analysis' in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        query_template (typing.Optional[str]): Template for query part to build the input prompt.\n        response_template (typing.Optional[str]): Template for response part to build the input prompt.\n        analysis_template (typing.Optional[str]): Template for analysis part to build the input prompt.\n        intensity_template (typing.Optional[str]): Template for intensity part to build the input prompt.\n        analysis_pattern (typing.Optional[str]): Pattern to parse the return sentiment analysis.\n        intensity_pattern (typing.Optional[str]): Pattern to parse the return sentiment intensity.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 15, "class_name": "dialog_topic_detection_mapper", "class_desc": "Mapper to generate user's topic labels in dialog. Input from history_key, query_key and response_key. Output lists of labels and analysis for queries in the dialog.", "arguments": "        api_model (<class 'str'>): API model name.\n        topic_candidates (typing.Optional[typing.List[str]]): The output topic candidates. Use open-domain topic labels if it is None.\n        max_round (typing.Annotated[int, Ge(ge=0)]): The max num of round in the dialog to build the prompt.\n        labels_key (<class 'str'>): The key name in the meta field to store the output labels. It is 'dialog_topic_labels' in default.\n        analysis_key (<class 'str'>): The key name in the meta field to store the corresponding analysis. It is 'dialog_topic_labels_analysis' in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        query_template (typing.Optional[str]): Template for query part to build the input prompt.\n        response_template (typing.Optional[str]): Template for response part to build the input prompt.\n        candidate_template (typing.Optional[str]): Template for topic candidates to build the input prompt.\n        analysis_template (typing.Optional[str]): Template for analysis part to build the input prompt.\n        labels_template (typing.Optional[str]): Template for labels part to build the input prompt.\n        analysis_pattern (typing.Optional[str]): Pattern to parse the return topic analysis.\n        labels_pattern (typing.Optional[str]): Pattern to parse the return topic labels.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 16, "class_name": "expand_macro_mapper", "class_desc": "Mapper to expand macro definitions in the document body of Latex samples.", "arguments": ""}, {"index": 17, "class_name": "extract_entity_attribute_mapper", "class_desc": "Extract attributes for given entities from the text", "arguments": "        api_model (<class 'str'>): API model name.\n        query_entities (typing.List[str]): Entity list to be queried.\n        query_attributes (typing.List[str]): Attribute list to be queried.\n        entity_key (<class 'str'>): The key name in the meta field to store the given main entity for attribute extraction. It's \"entity\" in default.\n        attribute_key (<class 'str'>): \n        attribute_desc_key (<class 'str'>): The key name in the meta field to store the extracted attribute description. It's \"attribute_description\" in default.\n        support_text_key (<class 'str'>): The key name in the meta field to store the attribute support text extracted from the raw text. It's \"support_text\" in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt_template (typing.Optional[str]): System prompt template for the task. Need to be specified by given entity and attribute.\n        input_template (typing.Optional[str]): Template for building the model input.\n        attr_pattern_template (typing.Optional[str]): Pattern for parsing the attribute from output. Need to be specified by given attribute. :param: demo_pattern: Pattern for parsing the demonstration from output to support the attribute.\n        demo_pattern (typing.Optional[str]): \n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 18, "class_name": "extract_entity_relation_mapper", "class_desc": "Extract entities and relations in the text for knowledge graph.", "arguments": "        api_model (<class 'str'>): API model name.\n        entity_types (typing.List[str]): Pre-defined entity types for knowledge graph.\n        entity_key (<class 'str'>): The key name to store the entities in the meta field. It's \"entity\" in default.\n        relation_key (<class 'str'>): The field name to store the relations between entities. It's \"relation\" in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        prompt_template (typing.Optional[str]): The template of input prompt.\n        tuple_delimiter (typing.Optional[str]): Delimiter to separate items in outputs.\n        record_delimiter (typing.Optional[str]): Delimiter to separate records in outputs.\n        completion_delimiter (typing.Optional[str]): To mark the end of the output.\n        max_gleaning (typing.Annotated[int, Ge(ge=0)]): the extra max num to call LLM to glean entities and relations.\n        continue_prompt (typing.Optional[str]): the prompt for gleaning entities and relations.\n        if_loop_prompt (typing.Optional[str]): the prompt to determine whether to stop gleaning.\n        entity_pattern (typing.Optional[str]): Regular expression for parsing entity record.\n        relation_pattern (typing.Optional[str]): Regular expression for parsing relation record.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 19, "class_name": "extract_event_mapper", "class_desc": "Extract events and relevant characters in the text", "arguments": "        api_model (<class 'str'>): API model name.\n        event_desc_key (<class 'str'>): The key name to store the event descriptions in the meta field. It's \"event_description\" in default.\n        relevant_char_key (<class 'str'>): The field name to store the relevant characters to the events in the meta field. It's \"relevant_characters\" in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        output_pattern (typing.Optional[str]): Regular expression for parsing model output.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 20, "class_name": "extract_keyword_mapper", "class_desc": "Generate keywords for the text", "arguments": "        api_model (<class 'str'>): API model name.\n        keyword_key (<class 'str'>): The key name to store the keywords in the meta field. It's \"keyword\" in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        prompt_template (typing.Optional[str]): The template of input prompt.\n        completion_delimiter (typing.Optional[str]): To mark the end of the output.\n        output_pattern (typing.Optional[str]): Regular expression for parsing keywords.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 21, "class_name": "extract_nickname_mapper", "class_desc": "Extract nickname relationship in the text.", "arguments": "        api_model (<class 'str'>): API model name.\n        nickname_key (<class 'str'>): The key name to store the nickname relationship in the meta field. It's \"nickname\" in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        output_pattern (typing.Optional[str]): Regular expression for parsing model output.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 22, "class_name": "extract_support_text_mapper", "class_desc": "Extract support sub text for a summary.", "arguments": "        api_model (<class 'str'>): API model name.\n        summary_key (<class 'str'>): The key name to store the input summary in the meta field. It's \"event_description\" in default.\n        support_text_key (<class 'str'>): The key name to store the output support text for the summary in the meta field. It's \"support_text\" in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 23, "class_name": "extract_tables_from_html_mapper", "class_desc": "Mapper to extract tables from HTML content.", "arguments": "        tables_field_name (<class 'str'>): Field name to store the extracted tables.\n        retain_html_tags (<class 'bool'>): If True, retains HTML tags in the tables; otherwise, removes them.\n        include_header (<class 'bool'>): If True, includes the table header; otherwise, excludes it. This parameter is effective only when `retain_html_tags` is False and applies solely to the extracted table content."}, {"index": 24, "class_name": "fix_unicode_mapper", "class_desc": "Mapper to fix unicode errors in text samples.", "arguments": "        normalization (<class 'str'>): the specified form of Unicode normalization mode, which can be one of ['NFC', 'NFKC', 'NFD', and 'NFKD'], default 'NFC'."}, {"index": 25, "class_name": "generate_qa_from_examples_mapper", "class_desc": "Mapper to generate question and answer pairs from examples. You should configure an empty dataset in your yaml config file: ``` generated_dataset_config:   type: 'EmptyFormatter'  # use `RayEmptyFormatter` when enable ray   length: ${The number of generated samples}   feature_keys: ${text key} ``` The number of samples generated is determined by the length of the empty dataset.", "arguments": "        hf_model (<class 'str'>): Huggingface model ID.\n        seed_file (<class 'str'>): Path to the seed file in chatml format.\n        example_num (typing.Annotated[int, Gt(gt=0)]): The number of selected examples. Randomly select N examples from \"seed_file\" and put them into prompt as QA examples.\n        similarity_threshold (<class 'float'>): The similarity score threshold between the generated samples and the seed examples. Range from 0 to 1. Samples with similarity score less than this threshold will be kept.\n        system_prompt (typing.Optional[str]): System prompt for guiding the generation task.\n        input_template (typing.Optional[str]): Template for building the input prompt. It must include one placeholder '{}', which will be replaced by `example_num` formatted examples defined by `example_template`.\n        example_template (typing.Optional[str]): Template for formatting one QA example. It must include one placeholder '{}', which will be replaced by one formatted qa_pair.\n        qa_pair_template (typing.Optional[str]): Template for formatting a single QA pair within each example. Must include two placeholders '{}' for the question and answer.\n        output_pattern (typing.Optional[str]): Regular expression pattern to extract questions and answers from model response.\n        enable_vllm (<class 'bool'>): Whether to use vllm for inference acceleration.\n        model_params (typing.Optional[typing.Dict]): Parameters for initializing the model.\n        sampling_params (typing.Optional[typing.Dict]): Sampling parameters for text generation. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 26, "class_name": "generate_qa_from_text_mapper", "class_desc": "Mapper to generate question and answer pairs from text. Recommended model list: [     'alibaba-pai/pai-llama3-8b-doc2qa',     'alibaba-pai/pai-baichuan2-7b-doc2qa',     'alibaba-pai/pai-qwen1_5-4b-doc2qa',     'alibaba-pai/pai-qwen1_5-7b-doc2qa',     'alibaba-pai/pai-qwen1_5-1b8-doc2qa',     'alibaba-pai/pai-qwen1_5-0b5-doc2qa' ] These recommended models are all trained with Chinese data and are suitable for Chinese.", "arguments": "        hf_model (<class 'str'>): Huggingface model ID.\n        max_num (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): The max num of returned QA sample for each text. Not limit if it is None.\n        output_pattern (typing.Optional[str]): Regular expression pattern to extract questions and answers from model response.\n        enable_vllm (<class 'bool'>): Whether to use vllm for inference acceleration.\n        model_params (typing.Optional[typing.Dict]): Parameters for initializing the model.\n        sampling_params (typing.Optional[typing.Dict]): Sampling parameters for text generation, e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 27, "class_name": "human_preference_annotation_mapper", "class_desc": "Operator for human preference annotation using Label Studio.", "arguments": "        label_config_file (<class 'str'>): \n        answer1_key (<class 'str'>): \n        answer2_key (<class 'str'>): \n        prompt_key (<class 'str'>): \n        chosen_key (<class 'str'>): \n        rejected_key (<class 'str'>): "}, {"index": 28, "class_name": "image_blur_mapper", "class_desc": "Mapper to blur images.", "arguments": "        p (<class 'float'>): Probability of the image being blurred.\n        blur_type (<class 'str'>): Type of blur kernel, including ['mean', 'box', 'gaussian'].\n        radius (<class 'float'>): Radius of blur kernel."}, {"index": 29, "class_name": "image_captioning_from_gpt4v_mapper", "class_desc": "Mapper to generate samples whose texts are generated based on gpt-4-vision and the image.", "arguments": "        mode (<class 'str'>): mode of text generated from images, can be one of ['reasoning', 'description', 'conversation', 'custom']\n        api_key (<class 'str'>): the API key to authenticate the request.\n        max_token (<class 'int'>): the maximum number of tokens to generate. Default is 500.\n        temperature (typing.Annotated[float, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=0), Le(le=1)])]): controls the randomness of the output (range from 0 to 1). Default is 0.\n        system_prompt (<class 'str'>): a string prompt used to set the context of a conversation and provide global guidance or rules for the gpt4-vision so that it can  generate responses in the expected way. If `mode` set to `custom`, the parameter will be used.\n        user_prompt (<class 'str'>): a string prompt to guide the generation of gpt4-vision for each samples. It's \"\" in default, which means no prompt provided.\n        user_prompt_key (typing.Optional[str]): the key name of fields in samples to store prompts for each sample. It's used for set different prompts for different samples. If it's none, use prompt in parameter \"prompt\". It's None in default.\n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only generated text in the final datasets and the original text will be removed. It's True in default.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 30, "class_name": "image_captioning_mapper", "class_desc": "Mapper to generate samples whose captions are generated based on another model and the figure.", "arguments": "        hf_img2seq (<class 'str'>): model name on huggingface to generate caption\n        trust_remote_code (<class 'bool'>): \n        caption_num (typing.Annotated[int, Gt(gt=0)]): how many candidate captions to generate for each image\n        keep_candidate_mode (<class 'str'>): retain strategy for the generated $caption_num$ candidates.  'random_any': Retain the random one from generated captions  'similar_one_simhash': Retain the generated one that is most similar to the original caption  'all': Retain all generated captions by concatenation  Note: This is a batched_OP, whose input and output type are both list. Suppose there are $N$ list of input samples, whose batch size is $b$, and denote caption_num as $M$. The number of total samples after generation is $2Nb$ when keep_original_sample is True and $Nb$ when keep_original_sample is False. For 'random_any' and 'similar_one_simhash' mode, it's $(1+M)Nb$ for 'all' mode when keep_original_sample is True and $MNb$ when keep_original_sample is False. \n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only generated captions in the final datasets and the original captions will be removed. It's True in default.\n        prompt (typing.Optional[str]): a string prompt to guide the generation of blip2 model for all samples globally. It's None in default, which means no prompt provided.\n        prompt_key (typing.Optional[str]): the key name of fields in samples to store prompts for each sample. It's used for set different prompts for different samples. If it's none, use prompt in parameter \"prompt\". It's None in default."}, {"index": 31, "class_name": "image_diffusion_mapper", "class_desc": "Generate image by diffusion model", "arguments": "        hf_diffusion (<class 'str'>): diffusion model name on huggingface to generate the image.\n        trust_remote_code (<class 'bool'>): \n        torch_dtype (<class 'str'>): the floating point type used to load the diffusion model. Can be one of ['fp32', 'fp16', 'bf16']\n        revision (<class 'str'>): The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier allowed by Git.\n        strength (typing.Annotated[float, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=0), Le(le=1)])]): Indicates extent to transform the reference image. Must be between 0 and 1. image is used as a starting point and more noise is added the higher the strength. The number of denoising steps depends on the amount of noise initially added. When strength is 1, added noise is maximum and the denoising process runs for the full number of iterations specified in num_inference_steps. A value of 1 essentially ignores image.\n        guidance_scale (<class 'float'>): A higher guidance scale value encourages the model to generate images closely linked to the text prompt at the expense of lower image quality. Guidance scale is enabled when guidance_scale > 1.\n        aug_num (typing.Annotated[int, Gt(gt=0)]): The image number to be produced by stable-diffusion model.\n        keep_original_sample (<class 'bool'>): \n        caption_key (typing.Optional[str]): the key name of fields in samples to store captions for each images. It can be a string if there is only one image in each sample. Otherwise, it should be a list. If it's none, ImageDiffusionMapper will produce captions for each images.\n        hf_img2seq (<class 'str'>): model name on huggingface to generate caption if caption_key is None."}, {"index": 32, "class_name": "image_face_blur_mapper", "class_desc": "Mapper to blur faces detected in images.", "arguments": "        cv_classifier (<class 'str'>): OpenCV classifier path for face detection. By default, we will use 'haarcascade_frontalface_alt.xml'.\n        blur_type (<class 'str'>): Type of blur kernel, including ['mean', 'box', 'gaussian'].\n        radius (typing.Annotated[float, Ge(ge=0)]): Radius of blur kernel."}, {"index": 33, "class_name": "image_remove_background_mapper", "class_desc": "Mapper to remove background of images", "arguments": "        alpha_matting (<class 'bool'>): \n        alpha_matting_foreground_threshold (<class 'int'>): \n        alpha_matting_background_threshold (<class 'int'>): \n        alpha_matting_erode_size (<class 'int'>): \n        bgcolor (typing.Optional[typing.Tuple[int, int, int, int]]): "}, {"index": 34, "class_name": "image_segment_mapper", "class_desc": "Perform segment-anything on images and return the bounding boxes.", "arguments": "        imgsz (unknown): resolution for image resizing\n        conf (unknown): confidence score threshold\n        iou (unknown): IoU (Intersection over Union) score threshold\n        model_path (unknown): the path to the FastSAM model. Model name should be one of ['FastSAM-x.pt', 'FastSAM-s.pt']."}, {"index": 35, "class_name": "image_tagging_mapper", "class_desc": "Mapper to generate image tags.", "arguments": "        tag_field_name (<class 'str'>): the field name to store the tags. It's \"image_tags\" in default."}, {"index": 36, "class_name": "mllm_mapper", "class_desc": "Mapper to use MLLMs for visual question answering tasks. Recommended model list: [     llava-hf/llava-v1.6-vicuna-7b-hf,     Qwen/Qwen2-VL-7B-Instruct, ]", "arguments": "        hf_model (<class 'str'>): hugginface model id.\n        max_new_tokens (unknown): the maximum number of new tokens generated by the model.\n        temperature (unknown): used to control the randomness of             generated text. The higher the temperature, the more                 random and creative the generated text will be.\n        top_p (unknown): randomly select the next word from the group             of words whose cumulative probability reaches p.\n        num_beams (unknown): the larger the beam search size, the higher             the quality of the generated text."}, {"index": 37, "class_name": "nlpaug_en_mapper", "class_desc": "Mapper to simply augment samples in English based on nlpaug library.", "arguments": "        sequential (<class 'bool'>): whether combine all augmentation methods to a sequence. If it's True, a sample will be augmented by all opened augmentation methods sequentially. If it's False, each opened augmentation method would generate its augmented samples independently.\n        aug_num (typing.Annotated[int, Gt(gt=0)]): number of augmented samples to be generated. If `sequential` is True, there will be total aug_num augmented samples generated. If it's False, there will be (aug_num * #opened_aug_method) augmented samples generated.\n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only generated texts in the final datasets and the original texts will be removed. It's True in default.\n        delete_random_word (<class 'bool'>): whether to open the augmentation method of deleting random words from the original texts. e.g. \"I love LLM\" --> \"I LLM\"\n        swap_random_word (<class 'bool'>): whether to open the augmentation method of swapping random contiguous words in the original texts. e.g. \"I love LLM\" --> \"Love I LLM\"\n        spelling_error_word (<class 'bool'>): whether to open the augmentation method of simulating the spelling error for words in the original texts. e.g. \"I love LLM\" --> \"Ai love LLM\"\n        split_random_word (<class 'bool'>): whether to open the augmentation method of splitting words randomly with whitespaces in the original texts. e.g. \"I love LLM\" --> \"I love LL M\"\n        keyboard_error_char (<class 'bool'>): whether to open the augmentation method of simulating the keyboard error for characters in the original texts. e.g. \"I love LLM\" --> \"I ;ov4 LLM\"\n        ocr_error_char (<class 'bool'>): whether to open the augmentation method of simulating the OCR error for characters in the original texts. e.g. \"I love LLM\" --> \"I 10ve LLM\"\n        delete_random_char (<class 'bool'>): whether to open the augmentation method of deleting random characters from the original texts. e.g. \"I love LLM\" --> \"I oe LLM\"\n        swap_random_char (<class 'bool'>): whether to open the augmentation method of swapping random contiguous characters in the original texts. e.g. \"I love LLM\" --> \"I ovle LLM\"\n        insert_random_char (<class 'bool'>): whether to open the augmentation method of inserting random characters into the original texts. e.g. \"I love LLM\" --> \"I ^lKove LLM\""}, {"index": 38, "class_name": "nlpcda_zh_mapper", "class_desc": "Mapper to simply augment samples in Chinese based on nlpcda library.", "arguments": "        sequential (<class 'bool'>): whether combine all augmentation methods to a sequence. If it's True, a sample will be augmented by all opened augmentation methods sequentially. If it's False, each opened augmentation method would generate its augmented samples independently.\n        aug_num (typing.Annotated[int, Gt(gt=0)]): number of augmented samples to be generated. If `sequential` is True, there will be total aug_num augmented samples generated. If it's False, there will be (aug_num * #opened_aug_method) augmented samples generated.\n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only generated texts in the final datasets and the original texts will be removed. It's True in default.\n        replace_similar_word (<class 'bool'>): whether to open the augmentation method of replacing random words with their similar words in the original texts. e.g. \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\" --> \"\u8fd9\u8fb9\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\"\n        replace_homophone_char (<class 'bool'>): whether to open the augmentation method of replacing random characters with their homophones in the original texts. e.g. \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\" --> \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6fd6\u636e\u589e\u5f3a\u65b9\u6cd5\"\n        delete_random_char (<class 'bool'>): whether to open the augmentation method of deleting random characters from the original texts. e.g. \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\" --> \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\"\n        swap_random_char (<class 'bool'>): whether to open the augmentation method of swapping random contiguous characters in the original texts. e.g. \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\" --> \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u5f3a\u589e\u65b9\u6cd5\"\n        replace_equivalent_num (<class 'bool'>): whether to open the augmentation method of replacing random numbers with their equivalent representations in the original texts. **Notice**: Only for numbers for now. e.g. \"\u8fd9\u91cc\u4e00\u5171\u67095\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\" --> \"\u8fd9\u91cc\u4e00\u5171\u6709\u4f0d\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\""}, {"index": 39, "class_name": "optimize_qa_mapper", "class_desc": "Mapper to optimize question-answer pairs.", "arguments": "        hf_model (<class 'str'>): Hugging Face model ID.\n        system_prompt (typing.Optional[str]): System prompt for guiding the optimization task.\n        input_template (typing.Optional[str]): Template for building the input for the model. Please make sure the template contains one placeholder '{}', which corresponds to the question and answer pair generated by param `qa_pair_template`.\n        qa_pair_template (typing.Optional[str]): Template for formatting the question and answer pair. Please make sure the template contains two '{}' to format question and answer.\n        output_pattern (typing.Optional[str]): Regular expression pattern to extract question and answer from model response.\n        enable_vllm (<class 'bool'>): Whether to use VLLM for inference acceleration.\n        model_params (typing.Optional[typing.Dict]): Parameters for initializing the model.\n        sampling_params (typing.Optional[typing.Dict]): Sampling parameters for text generation (e.g., {'temperature': 0.9, 'top_p': 0.95})."}, {"index": 40, "class_name": "optimize_query_mapper", "class_desc": "Mapper to optimize query in question-answer pairs.", "arguments": "        hf_model (<class 'str'>): Hugging Face model ID.\n        system_prompt (typing.Optional[str]): System prompt for guiding the optimization task.\n        input_template (typing.Optional[str]): Template for building the input for the model. Please make sure the template contains one placeholder '{}', which corresponds to the question and answer pair generated by param `qa_pair_template`.\n        qa_pair_template (typing.Optional[str]): Template for formatting the question and answer pair. Please make sure the template contains two '{}' to format question and answer.\n        output_pattern (typing.Optional[str]): Regular expression pattern to extract question and answer from model response.\n        enable_vllm (<class 'bool'>): Whether to use VLLM for inference acceleration.\n        model_params (typing.Optional[typing.Dict]): Parameters for initializing the model.\n        sampling_params (typing.Optional[typing.Dict]): Sampling parameters for text generation (e.g., {'temperature': 0.9, 'top_p': 0.95})."}, {"index": 41, "class_name": "optimize_response_mapper", "class_desc": "Mapper to optimize response in question-answer pairs.", "arguments": "        hf_model (<class 'str'>): Hugging Face model ID.\n        system_prompt (typing.Optional[str]): System prompt for guiding the optimization task.\n        input_template (typing.Optional[str]): Template for building the input for the model. Please make sure the template contains one placeholder '{}', which corresponds to the question and answer pair generated by param `qa_pair_template`.\n        qa_pair_template (typing.Optional[str]): Template for formatting the question and answer pair. Please make sure the template contains two '{}' to format question and answer.\n        output_pattern (typing.Optional[str]): Regular expression pattern to extract question and answer from model response.\n        enable_vllm (<class 'bool'>): Whether to use VLLM for inference acceleration.\n        model_params (typing.Optional[typing.Dict]): Parameters for initializing the model.\n        sampling_params (typing.Optional[typing.Dict]): Sampling parameters for text generation (e.g., {'temperature': 0.9, 'top_p': 0.95})."}, {"index": 42, "class_name": "pair_preference_mapper", "class_desc": "Mapper to construct paired preference samples.", "arguments": "        api_model (<class 'str'>): API model name.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt (typing.Optional[str]): System prompt for guiding the generation task.\n        input_template (typing.Optional[str]): Template for building the model input. It must contain placeholders '{query}' and '{response}', and can optionally include '{reference}'.\n        output_pattern (typing.Optional[str]): Regular expression for parsing model output.\n        rejected_key (<class 'str'>): The field name in the sample to store the generated rejected response. Defaults to 'rejected_response'.\n        reason_key (<class 'str'>): The field name in the sample to store the reason for generating the response. Defaults to 'reason'.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retries for the API call in case of response parsing failure. Defaults to 3.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 43, "class_name": "punctuation_normalization_mapper", "class_desc": "Mapper to normalize unicode punctuations to English punctuations in text samples.", "arguments": ""}, {"index": 44, "class_name": "python_file_mapper", "class_desc": "Mapper for executing Python function defined in a file.", "arguments": "        file_path (<class 'str'>): The path to the Python file containing the function to be executed.\n        function_name (<class 'str'>): The name of the function defined in the file to be executed.\n        batched (<class 'bool'>): A boolean indicating whether to process input data in batches."}, {"index": 45, "class_name": "python_lambda_mapper", "class_desc": "Mapper for executing Python lambda function on data samples.", "arguments": "        lambda_str (<class 'str'>): A string representation of the lambda function to be executed on data samples. If empty, the identity function is used.\n        batched (<class 'bool'>): A boolean indicating whether to process input data in batches."}, {"index": 46, "class_name": "query_sentiment_detection_mapper", "class_desc": "Mapper to predict user's sentiment label ('negative', 'neutral' and 'positive') in query. Input from query_key. Output label and corresponding score for the query, which is store in 'query_sentiment_label' and 'query_sentiment_label_score' in Data-Juicer meta field.", "arguments": "        hf_model (<class 'str'>): Huggingface model ID to predict sentiment label.\n        zh_to_en_hf_model (typing.Optional[str]): Translation model from Chinese to English. If not None, translate the query from Chinese to English.\n        model_params (typing.Dict): model param for hf_model.\n        zh_to_en_model_params (typing.Dict): model param for zh_to_hf_model.\n        label_key (<class 'str'>): The key name in the meta field to store the output label. It is 'query_sentiment_label' in default.\n        score_key (<class 'str'>): The key name in the meta field to store the corresponding label score. It is 'query_sentiment_label_score' in default."}, {"index": 47, "class_name": "query_intent_detection_mapper", "class_desc": "Mapper to predict user's Intent label in query. Input from query_key. Output intent label and corresponding score for the query.", "arguments": "        hf_model (<class 'str'>): Huggingface model ID to predict intent label.\n        zh_to_en_hf_model (typing.Optional[str]): Translation model from Chinese to English. If not None, translate the query from Chinese to English.\n        model_params (typing.Dict): model param for hf_model.\n        zh_to_en_model_params (typing.Dict): model param for zh_to_hf_model.\n        label_key (<class 'str'>): The key name in the meta field to store the output label. It is 'query_intent_label' in default.\n        score_key (<class 'str'>): The key name in the meta field to store the corresponding label score. It is 'query_intent_label_score' in default."}, {"index": 48, "class_name": "query_topic_detection_mapper", "class_desc": "Mapper to predict user's topic label in query. Input from query_key. Output topic label and corresponding score for the query, which is store in 'query_topic_label' and 'query_topic_label_score' in Data-Juicer meta field.", "arguments": "        hf_model (<class 'str'>): Huggingface model ID to predict topic label.\n        zh_to_en_hf_model (typing.Optional[str]): Translation model from Chinese to English. If not None, translate the query from Chinese to English.\n        model_params (typing.Dict): model param for hf_model.\n        zh_to_en_model_params (typing.Dict): model param for zh_to_hf_model.\n        label_key (<class 'str'>): The key name in the meta field to store the output label. It is 'query_topic_label' in default.\n        score_key (<class 'str'>): The key name in the meta field to store the corresponding label score. It is 'query_topic_label_score' in default."}, {"index": 49, "class_name": "relation_identity_mapper", "class_desc": "identify relation between two entity in the text.", "arguments": "        api_model (<class 'str'>): API model name.\n        source_entity (<class 'str'>): The source entity of the relation to be identified.\n        target_entity (<class 'str'>): The target entity of the relation to be identified.\n        output_key (<class 'str'>): The output key in the meta field in the samples. It is 'role_relation' in default.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        system_prompt_template (typing.Optional[str]): System prompt template for the task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        output_pattern_template (typing.Optional[str]): Regular expression template for parsing model output.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        drop_text (<class 'bool'>): If drop the text in the output.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 50, "class_name": "remove_bibliography_mapper", "class_desc": "Mapper to remove bibliography at the end of documents in Latex samples.", "arguments": ""}, {"index": 51, "class_name": "remove_comments_mapper", "class_desc": "Mapper to remove comments in different kinds of documents.  Only support 'tex' for now.", "arguments": "        doc_type (typing.Union[str, typing.List[str]]): Type of document to remove comments.\n        inline (<class 'bool'>): Whether to remove inline comments.\n        multiline (<class 'bool'>): Whether to remove multiline comments."}, {"index": 52, "class_name": "remove_header_mapper", "class_desc": "Mapper to remove headers at the beginning of documents in Latex samples.", "arguments": "        drop_no_head (<class 'bool'>): whether to drop sample texts without headers."}, {"index": 53, "class_name": "remove_long_words_mapper", "class_desc": "Mapper to remove long words within a specific range.", "arguments": "        min_len (<class 'int'>): The min mapper word length in this op, words will be filtered if their length is below this parameter.\n        max_len (<class 'int'>): The max mapper word length in this op, words will be filtered if their length exceeds this parameter."}, {"index": 54, "class_name": "remove_non_chinese_character_mapper", "class_desc": "Mapper to remove non chinese Character in text samples.", "arguments": "        keep_alphabet (<class 'bool'>): whether to keep alphabet\n        keep_number (<class 'bool'>): whether to keep number\n        keep_punc (<class 'bool'>): whether to keep punctuation"}, {"index": 55, "class_name": "remove_repeat_sentences_mapper", "class_desc": "Mapper to remove repeat sentences in text samples.", "arguments": "        lowercase (<class 'bool'>): Whether to convert sample text to lower case\n        ignore_special_character (<class 'bool'>): Whether to ignore special characters when judging repeated sentences. Special characters are all characters except Chinese characters, letters and numbers.\n        min_repeat_sentence_length (<class 'int'>): Sentences shorter than this length will not be deduplicated. If ignore_special_character is set to True, then special characters are not included in this length."}, {"index": 56, "class_name": "remove_specific_chars_mapper", "class_desc": "Mapper to clean specific chars in text samples.", "arguments": "        chars_to_remove (typing.Union[str, typing.List[str]]): a list or a string including all characters that need to be removed from text."}, {"index": 57, "class_name": "remove_table_text_mapper", "class_desc": "Mapper to remove table texts from text samples.  Regular expression is used to remove tables in the range of column number of tables.", "arguments": "        min_col (typing.Annotated[int, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=2), Le(le=20)])]): The min number of columns of table to remove.\n        max_col (typing.Annotated[int, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=2), Le(le=20)])]): The max number of columns of table to remove."}, {"index": 58, "class_name": "remove_words_with_incorrect_substrings_mapper", "class_desc": "Mapper to remove words with incorrect substrings.", "arguments": "        lang (<class 'str'>): sample in which language\n        tokenization (<class 'bool'>): whether to use model to tokenize documents\n        substrings (typing.Optional[typing.List[str]]): The incorrect substrings in words."}, {"index": 59, "class_name": "replace_content_mapper", "class_desc": "Mapper to replace all content in the text that matches a specific regular expression pattern with a designated replacement string.", "arguments": "        pattern (typing.Union[str, typing.List[str], NoneType]): regular expression pattern(s) to search for within text\n        repl (typing.Union[str, typing.List[str]]): replacement string(s), default is empty string"}, {"index": 60, "class_name": "sdxl_prompt2prompt_mapper", "class_desc": "Generate pairs of similar images by the SDXL model", "arguments": "        hf_diffusion (<class 'str'>): diffusion model name on huggingface to generate the image.\n        trust_remote_code (unknown): \n        torch_dtype (<class 'str'>): the floating point type used to load the diffusion model.\n        num_inference_steps (<class 'float'>): The larger the value, the better the image generation quality; however, this also increases the time required for generation.\n        guidance_scale (<class 'float'>): A higher guidance scale value encourages the model to generate images closely linked to the text prompt at the expense of lower image quality. Guidance scale is enabled when\n        text_key_second (unknown): used to store the first caption in the caption pair.\n        text_key_third (unknown): used to store the second caption in the caption pair."}, {"index": 61, "class_name": "sentence_augmentation_mapper", "class_desc": "Mapper to augment sentences. The purpose of this operation is to enhance sentences. If the input text is at the document level, the enhancement effect may not be optimal. Therefore, please consider the length of the input text carefully.  Recommended model list: [     lmsys/vicuna-13b-v1.5     Qwen/Qwen2-7B-Instruct ]", "arguments": "        hf_model (<class 'str'>): Huggingface model id.\n        system_prompt (<class 'str'>): System prompt.\n        task_sentence (<class 'str'>): The instruction for the current task.\n        max_new_tokens (unknown): the maximum number of new tokens generated by the model.\n        temperature (unknown): used to control the randomness of generated text. The higher the temperature, the more random and creative the generated text will be.\n        top_p (unknown): randomly select the next word from the group of words whose cumulative probability reaches p.\n        num_beams (unknown): the larger the beam search size, the higher the quality of the generated text."}, {"index": 62, "class_name": "sentence_split_mapper", "class_desc": "Mapper to split text samples to sentences.", "arguments": "        lang (<class 'str'>): split sentence of text in which language."}, {"index": 63, "class_name": "text_chunk_mapper", "class_desc": "Split input text to chunks.", "arguments": "        max_len (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): Split text into multi texts with this max len if not None.\n        split_pattern (typing.Optional[str]): Make sure split in this pattern if it is not None and force cut if the length exceeds max_len.\n        overlap_len (typing.Annotated[int, Ge(ge=0)]): Overlap length of the split texts if not split in the split pattern.\n        tokenizer (typing.Optional[str]): The tokenizer name of Hugging Face tokenizers. The text length will be calculate as the token num if it is offered. Otherwise, the text length equals to string length. Support tiktoken tokenizer (such as gpt-4o), dashscope tokenizer ( such as qwen2.5-72b-instruct) and huggingface tokenizer. :trust_remote_code: for loading huggingface model\n        trust_remote_code (<class 'bool'>): "}, {"index": 64, "class_name": "video_captioning_from_audio_mapper", "class_desc": "Mapper to caption a video according to its audio streams based on Qwen-Audio model.", "arguments": "        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only captioned sample in the final datasets and the original sample will be removed. It's True in default."}, {"index": 65, "class_name": "video_captioning_from_frames_mapper", "class_desc": "Mapper to generate samples whose captions are generated based on an image-to-text model and sampled video frames. Captions from different frames will be concatenated to a single string.", "arguments": "        hf_img2seq (<class 'str'>): model name on huggingface to generate caption\n        trust_remote_code (<class 'bool'>): \n        caption_num (typing.Annotated[int, Gt(gt=0)]): how many candidate captions to generate for each video\n        keep_candidate_mode (<class 'str'>): retain strategy for the generated $caption_num$ candidates.  'random_any': Retain the random one from generated captions  'similar_one_simhash': Retain the generated one that is most similar to the original caption  'all': Retain all generated captions by concatenation  Note: This is a batched_OP, whose input and output type are both list. Suppose there are $N$ list of input samples, whose batch size is $b$, and denote caption_num as $M$. The number of total samples after generation is $2Nb$ when keep_original_sample is True and $Nb$ when keep_original_sample is False. For 'random_any' and 'similar_one_simhash' mode, it's $(1+M)Nb$ for 'all' mode when keep_original_sample is True and $MNb$ when keep_original_sample is False. \n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only generated captions in the final datasets and the original captions will be removed. It's True in default.\n        prompt (typing.Optional[str]): a string prompt to guide the generation of image-to-text model for all samples globally. It's None in default, which means no prompt provided.\n        prompt_key (typing.Optional[str]): the key name of fields in samples to store prompts for each sample. It's used for set different prompts for different samples. If it's none, use prompt in parameter \"prompt\". It's None in default.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame videos from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        horizontal_flip (<class 'bool'>): flip frame video horizontally (left to right).\n        vertical_flip (<class 'bool'>): flip frame video vertically (top to bottom)."}, {"index": 66, "class_name": "video_captioning_from_summarizer_mapper", "class_desc": "Mapper to generate video captions by summarizing several kinds of generated texts (captions from video/audio/frames, tags from audio/frames, ...)", "arguments": "        hf_summarizer (<class 'str'>): the summarizer model used to summarize texts generated by other methods.\n        trust_remote_code (<class 'bool'>): \n        consider_video_caption_from_video (<class 'bool'>): whether to consider the video caption generated from video directly in the summarization process. Default: True.\n        consider_video_caption_from_audio (<class 'bool'>): whether to consider the video caption generated from audio streams in the video in the summarization process. Default: True.\n        consider_video_caption_from_frames (<class 'bool'>): whether to consider the video caption generated from sampled frames from the video in the summarization process. Default: True.\n        consider_video_tags_from_audio (<class 'bool'>): whether to consider the video tags generated from audio streams in the video in the summarization process. Default: True.\n        consider_video_tags_from_frames (<class 'bool'>): whether to consider the video tags generated from sampled frames from the video in the summarization process. Default: True.\n        vid_cap_from_vid_args (typing.Optional[typing.Dict]): the arg dict for video captioning from video directly with keys are the arg names and values are the arg values. Default: None.\n        vid_cap_from_frm_args (typing.Optional[typing.Dict]): the arg dict for video captioning from sampled frames from the video with keys are the arg names and values are the arg values. Default: None.\n        vid_tag_from_aud_args (typing.Optional[typing.Dict]): the arg dict for video tagging from audio streams in the video with keys are the arg names and values are the arg values. Default: None.\n        vid_tag_from_frm_args (typing.Optional[typing.Dict]): the arg dict for video tagging from sampled frames from the video with keys are the arg names and values are the arg values. Default: None.\n        keep_tag_num (typing.Annotated[int, Gt(gt=0)]): max number N of tags from sampled frames to keep. Too many tags might bring negative influence to summarized text, so we consider to only keep the N most frequent tags. Default: 5.\n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only summarized captions in the final datasets and the original captions will be removed. It's True in default."}, {"index": 67, "class_name": "video_captioning_from_video_mapper", "class_desc": "Mapper to generate samples whose captions are generated based on a video-to-text model and sampled video frame.", "arguments": "        hf_video_blip (<class 'str'>): video-blip model name on huggingface to generate caption\n        trust_remote_code (<class 'bool'>): \n        caption_num (typing.Annotated[int, Gt(gt=0)]): how many candidate captions to generate for each video\n        keep_candidate_mode (<class 'str'>): retain strategy for the generated $caption_num$ candidates.  'random_any': Retain the random one from generated captions  'similar_one_simhash': Retain the generated one that is most similar to the original caption  'all': Retain all generated captions by concatenation  Note: This is a batched_OP, whose input and output type are both list. Suppose there are $N$ list of input samples, whose batch size is $b$, and denote caption_num as $M$. The number of total samples after generation is $2Nb$ when keep_original_sample is True and $Nb$ when keep_original_sample is False. For 'random_any' and 'similar_one_simhash' mode, it's $(1+M)Nb$ for 'all' mode when keep_original_sample is True and $MNb$ when keep_original_sample is False. \n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only generated captions in the final datasets and the original captions will be removed. It's True in default.\n        prompt (typing.Optional[str]): a string prompt to guide the generation of video-blip model for all samples globally. It's None in default, which means no prompt provided.\n        prompt_key (typing.Optional[str]): the key name of fields in samples to store prompts for each sample. It's used for set different prompts for different samples. If it's none, use prompt in parameter \"prompt\". It's None in default.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame videos from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        horizontal_flip (<class 'bool'>): flip frame video horizontally (left to right).\n        vertical_flip (<class 'bool'>): flip frame video vertically (top to bottom)."}, {"index": 68, "class_name": "video_extract_frames_mapper", "class_desc": "Mapper to extract frames from video files according to specified methods. Extracted Frames Data Format:     The data format for the extracted frames is a dictionary mapping     video key to extracted frames directory where the extracted     frames are saved. The dictionary follows the structure:     {         \"video_key_1\": \"/${frame_dir}/video_key_1_filename/\",         \"video_key_2\": \"/${frame_dir}/video_key_2_filename/\",         ...     }", "arguments": "        frame_sampling_method (<class 'str'>): sampling method of extracting frame videos from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. If \"duration\" > 0, frame_sampling_method acts on every segment. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration. If \"duration\" > 0, frame_num is the number of frames per segment.\n        duration (<class 'float'>): The duration of each segment in seconds. If 0, frames are extracted from the entire video. If duration > 0, the video is segmented into multiple segments based on duration, and frames are extracted from each segment.\n        frame_dir (<class 'str'>): Output directory to save extracted frames. If None, a default directory based on the video file path is used.\n        frame_key (unknown): The name of field to save generated frames info."}, {"index": 69, "class_name": "video_ffmpeg_wrapped_mapper", "class_desc": "Simple wrapper for FFmpeg video filters.", "arguments": "        filter_name (typing.Optional[str]): ffmpeg video filter name.\n        filter_kwargs (typing.Optional[typing.Dict]): keyword-arguments passed to ffmpeg filter.\n        global_args (typing.Optional[typing.List[str]]): list-arguments passed to ffmpeg command-line.\n        capture_stderr (<class 'bool'>): whether to capture stderr.\n        overwrite_output (<class 'bool'>): whether to overwrite output file."}, {"index": 70, "class_name": "video_face_blur_mapper", "class_desc": "Mapper to blur faces detected in videos.", "arguments": "        cv_classifier (<class 'str'>): OpenCV classifier path for face detection. By default, we will use 'haarcascade_frontalface_alt.xml'.\n        blur_type (<class 'str'>): Type of blur kernel, including ['mean', 'box', 'gaussian'].\n        radius (<class 'float'>): Radius of blur kernel."}, {"index": 71, "class_name": "video_remove_watermark_mapper", "class_desc": "Remove the watermarks in videos given regions.", "arguments": "        roi_strings (typing.List[str]): a given list of regions the watermarks locate. The format of each can be \"x1, y1, x2, y2\", \"(x1, y1, x2, y2)\", or \"[x1, y1, x2, y2]\".\n        roi_type (<class 'str'>): the roi string type. When the type is 'pixel', (x1, y1), (x2, y2) are the locations of pixels in the top left corner and the bottom right corner respectively. If the roi_type is 'ratio', the coordinates are normalized by widths and heights.\n        roi_key (typing.Optional[str]): the key name of fields in samples to store roi_strings for each sample. It's used for set different rois for different samples. If it's none, use rois in parameter \"roi_strings\". It's None in default.\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video to detect the pixels of watermark.\n        min_frame_threshold (typing.Annotated[int, Gt(gt=0)]): a coordination is considered as the location of a watermark pixel when it is that in no less min_frame_threshold frames.\n        detection_method (<class 'str'>): the method to detect the pixels of watermark. If it is 'pixel_value', we consider the distribution of pixel value in each frame. If it is 'pixel_diversity', we will consider the pixel diversity in different frames. The min_frame_threshold is useless and frame_num must be greater than 1 in 'pixel_diversity' mode."}, {"index": 72, "class_name": "video_resize_aspect_ratio_mapper", "class_desc": "Mapper to resize videos by aspect ratio. AspectRatio = W / H.", "arguments": "        min_ratio (<class 'str'>): The minimum aspect ratio to enforce videos with an aspect ratio below `min_ratio` will be resized to match this minimum ratio. The ratio should be provided as a string in the format \"9:21\" or \"9/21\".\n        max_ratio (<class 'str'>): The maximum aspect ratio to enforce videos with an aspect ratio above `max_ratio` will be resized to match this maximum ratio. The ratio should be provided as a string in the format \"21:9\" or \"21/9\".\n        strategy (<class 'str'>): The resizing strategy to apply when adjusting the video dimensions. It can be either 'decrease' to reduce the dimension or 'increase' to enlarge it. Accepted values are ['decrease', 'increase']."}, {"index": 73, "class_name": "video_resize_resolution_mapper", "class_desc": "Mapper to resize videos resolution. We leave the super resolution with deep learning for future works.", "arguments": "        min_width (<class 'int'>): Videos with width less than 'min_width' will be mapped to videos with equal or bigger width.\n        max_width (<class 'int'>): Videos with width more than 'max_width' will be mapped to videos with equal of smaller width.\n        min_height (<class 'int'>): Videos with height less than 'min_height' will be mapped to videos with equal or bigger height.\n        max_height (<class 'int'>): Videos with height more than 'max_height' will be mapped to videos with equal or smaller height.\n        force_original_aspect_ratio (<class 'str'>): Enable decreasing or             increasing output video width or height if necessary             to keep the original aspect ratio, including ['disable',             'decrease', 'increase'].\n        force_divisible_by (typing.Annotated[int, Gt(gt=0)]): Ensures that both the output dimensions,             width and height, are divisible by the given integer when used             together with force_original_aspect_ratio, must be a positive             even number."}, {"index": 74, "class_name": "video_split_by_duration_mapper", "class_desc": "Mapper to split video by duration.", "arguments": "        split_duration (<class 'float'>): duration of each video split in seconds.\n        min_last_split_duration (<class 'float'>): The minimum allowable duration in seconds for the last video split. If the duration of the last split is less than this value, it will be discarded.\n        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only cut sample in the final datasets and the original sample will be removed. It's True in default."}, {"index": 75, "class_name": "video_split_by_key_frame_mapper", "class_desc": "Mapper to split video by key frame.", "arguments": "        keep_original_sample (<class 'bool'>): whether to keep the original sample. If it's set to False, there will be only split sample in the final datasets and the original sample will be removed. It's True in default."}, {"index": 76, "class_name": "video_split_by_scene_mapper", "class_desc": "Mapper to cut videos into scene clips.", "arguments": "        detector (<class 'str'>): Algorithm from `scenedetect.detectors`. Should be one of ['ContentDetector', 'ThresholdDetector', 'AdaptiveDetector`].\n        threshold (typing.Annotated[float, Ge(ge=0)]): Threshold passed to the detector.\n        min_scene_len (typing.Annotated[int, Ge(ge=0)]): Minimum length of any scene.\n        show_progress (<class 'bool'>): Whether to show progress from scenedetect."}, {"index": 77, "class_name": "video_tagging_from_audio_mapper", "class_desc": "Mapper to generate video tags from audio streams extracted by video using the Audio Spectrogram Transformer.", "arguments": "        hf_ast (<class 'str'>): path to the HF model to tag from audios.\n        trust_remote_code (<class 'bool'>): whether to trust the remote code of HF models\n        tag_field_name (<class 'str'>): the field name to store the tags. It's \"video_audio_tags\" in default."}, {"index": 78, "class_name": "video_tagging_from_frames_mapper", "class_desc": "Mapper to generate video tags from frames extract by video.", "arguments": "        frame_sampling_method (<class 'str'>): sampling method of extracting frame images from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        tag_field_name (<class 'str'>): the field name to store the tags. It's \"video_frame_tags\" in default."}, {"index": 79, "class_name": "whitespace_normalization_mapper", "class_desc": "Mapper to normalize different kinds of whitespaces to whitespace ' ' (0x20) in text samples.  Different kinds of whitespaces can be found here: https://en.wikipedia.org/wiki/Whitespace_character", "arguments": ""}, {"index": 80, "class_name": "alphanumeric_filter", "class_desc": "Filter to keep samples with alphabet/numeric ratio within a specific range.", "arguments": "        tokenization (<class 'bool'>): Whether to count the ratio of alphanumeric to the total number of tokens. if tokenization=False, it will count the ratio of alphanumeric to the total number of characters.\n        min_ratio (<class 'float'>): The min filter ratio in alphanumeric op, samples will be filtered if their alphabet/numeric ratio is below this parameter.\n        max_ratio (<class 'float'>): The max filter ratio in alphanumeric op, samples will be filtered if their alphabet/numeric ratio exceeds this parameter."}, {"index": 81, "class_name": "audio_duration_filter", "class_desc": "Keep data samples whose audios' durations are within a specified range.", "arguments": "        min_duration (<class 'int'>): The min audio duration to keep samples in seconds. It's 0 by default.\n        max_duration (<class 'int'>): The max audio duration to keep samples in seconds. It's sys.maxsize by default.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all audios. 'any': keep this sample if any audios meet the condition. 'all': keep this sample only if all audios meet the condition."}, {"index": 82, "class_name": "audio_nmf_snr_filter", "class_desc": "Keep data samples whose audios' SNRs (computed based on NMF) are within a specified range.", "arguments": "        min_snr (<class 'float'>): The min audio SNR to keep samples in dB. It's 0 by default.\n        max_snr (<class 'float'>): The max audio SNR to keep samples in dB. It's sys.maxsize by default.\n        nmf_iter_num (typing.Annotated[int, Gt(gt=0)]): The max number of iterations to run NMF. It's 500 in default.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all audios. 'any': keep this sample if any audios meet the condition. 'all': keep this sample only if all audios meet the condition."}, {"index": 83, "class_name": "audio_size_filter", "class_desc": "Keep data samples whose audio size (in bytes/kb/MB/...) within a specific range.", "arguments": "        min_size (<class 'str'>): The min audio size to keep samples.  set to be \"0\" by default for no size constraint\n        max_size (<class 'str'>): The max audio size to keep samples.  set to be \"1Tb\" by default, an approximate for un-limited case\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all audios. 'any': keep this sample if any audios meet the condition. 'all': keep this sample only if all audios meet the condition."}, {"index": 84, "class_name": "average_line_length_filter", "class_desc": "Filter to keep samples with average line length within a specific range.", "arguments": "        min_len (<class 'int'>): The min filter length in this op, samples will be filtered if their average line length is below this parameter.\n        max_len (<class 'int'>): The max filter length in this op, samples will be filtered if their average line length exceeds this parameter."}, {"index": 85, "class_name": "character_repetition_filter", "class_desc": "Filter to keep samples with char-level n-gram repetition ratio within a specific range.", "arguments": "        rep_len (typing.Annotated[int, Gt(gt=0)]): Repetition length for char-level n-gram.\n        min_ratio (<class 'float'>): The min filter ratio in this op, samples will be filtered if their char-level n-gram repetition ratio is below this parameter.\n        max_ratio (<class 'float'>): The max filter ratio in this op, samples will be filtered if their char-level n-gram repetition ratio exceeds this parameter."}, {"index": 86, "class_name": "flagged_words_filter", "class_desc": "Filter to keep samples with flagged-word ratio less than a specific max value.", "arguments": "        lang (<class 'str'>): Consider flagged words in what language. If lang == \"all\", we will adopt the one merged from all the available languages\n        tokenization (<class 'bool'>): Whether to use model to tokenize documents\n        max_ratio (<class 'float'>): The max filter ratio in this op.\n        flagged_words_dir (<class 'str'>): The directory storing the flagged_words file(s) whose name includes \"flagged_words\" and in json format\n        use_words_aug (<class 'bool'>): Whether to augment words, especially for Chinese and Vietnamese\n        words_aug_group_sizes (typing.List[typing.Annotated[int, Gt(gt=0)]]): The group size of words to augment\n        words_aug_join_char (<class 'str'>): The join char between words to augment"}, {"index": 87, "class_name": "image_aesthetics_filter", "class_desc": "Filter to keep samples with aesthetics scores within a specific range.", "arguments": "        hf_scorer_model (<class 'str'>): Huggingface model name for the aesthetics predictor. By default, we will use 'shunk031/aesthetics-predictor-v2-sac-logos-ava1-l14-linearMSE', refer to pypi.org/project/simple-aesthetics-predictor\n        trust_remote_code (<class 'bool'>): \n        min_score (<class 'float'>): Min score for the predicted aesthetics in an image.\n        max_score (<class 'float'>): Max score for the predicted aesthetics in an image.\n        any_or_all (<class 'str'>): Keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 88, "class_name": "image_aspect_ratio_filter", "class_desc": "Filter to keep samples with image aspect ratio within a specific range. AspectRatio = W / H.", "arguments": "        min_ratio (<class 'float'>): The min aspect ratio to keep samples.\n        max_ratio (<class 'float'>): The max aspect ratio to keep samples.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 89, "class_name": "image_face_count_filter", "class_desc": "Filter to keep samples with the number of faces within a specific range.", "arguments": "        cv_classifier (<class 'str'>): OpenCV classifier path for face detection. By default, we will use 'haarcascade_frontalface_alt.xml'.\n        min_face_count (<class 'int'>): Minimum number of faces required for samples.\n        max_face_count (<class 'int'>): Maximum number of faces required for samples.\n        any_or_all (<class 'str'>): Keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 90, "class_name": "image_face_ratio_filter", "class_desc": "Filter to keep samples with face area ratios within a specific range.", "arguments": "        cv_classifier (<class 'str'>): OpenCV classifier path for face detection. By default, we will use 'haarcascade_frontalface_alt.xml'.\n        min_ratio (<class 'float'>): Min ratio for the largest face area in an image.\n        max_ratio (<class 'float'>): Max ratio for the largest face area in an image.\n        any_or_all (<class 'str'>): Keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 91, "class_name": "image_nsfw_filter", "class_desc": "Filter to keep samples whose images have low nsfw scores.", "arguments": "        hf_nsfw_model (<class 'str'>): nsfw detection model name on huggingface.\n        trust_remote_code (<class 'bool'>): \n        max_score (<class 'float'>): the nsfw score threshold for samples. range from 0 to 1. Samples with nsfw score less than this threshold will be kept.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 92, "class_name": "image_pair_similarity_filter", "class_desc": "Filter to keep image pairs with similarities between images within a specific range.", "arguments": "        hf_clip (unknown): clip model name on huggingface to compute the similarity between image and text.\n        trust_remote_code (unknown): \n        min_score (<class 'jsonargparse.typing.ClosedUnitInterval'>): The min similarity to keep samples.\n        max_score (<class 'jsonargparse.typing.ClosedUnitInterval'>): The max similarity to keep samples.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 93, "class_name": "image_shape_filter", "class_desc": "Filter to keep samples with image shape (w, h) within specific ranges.", "arguments": "        min_width (<class 'int'>): The min width to keep samples.\n        max_width (<class 'int'>): The max width to keep samples.\n        min_height (<class 'int'>): The min height to keep samples.\n        max_height (<class 'int'>): The max height to keep samples.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 94, "class_name": "image_size_filter", "class_desc": "Keep data samples whose image size (in Bytes/KB/MB/...) within a specific range.", "arguments": "        min_size (<class 'str'>): The min image size to keep samples.  set to be \"0\" by default for no size constraint\n        max_size (<class 'str'>): The max image size to keep samples.  set to be \"1TB\" by default, an approximate for un-limited case\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 95, "class_name": "image_text_matching_filter", "class_desc": "Filter to keep samples those matching score between image and text within a specific range.", "arguments": "        hf_blip (<class 'str'>): blip model name on huggingface to compute the matching score between image and text.\n        trust_remote_code (<class 'bool'>): \n        min_score (<class 'float'>): The min matching score to keep samples.\n        max_score (<class 'float'>): The max matching score to keep samples.\n        horizontal_flip (<class 'bool'>): Flip image horizontally (left to right).\n        vertical_flip (<class 'bool'>): Flip image vertically (top to bottom).\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition.\n        reduce_mode (<class 'str'>): reduce mode when one text corresponds to multiple images in a chunk. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values"}, {"index": 96, "class_name": "image_text_similarity_filter", "class_desc": "Filter to keep samples those similarities between image and text within a specific range.", "arguments": "        hf_clip (<class 'str'>): clip model name on huggingface to compute the similarity between image and text.\n        trust_remote_code (<class 'bool'>): \n        min_score (<class 'float'>): The min similarity to keep samples.\n        max_score (<class 'float'>): The max similarity to keep samples.\n        horizontal_flip (<class 'bool'>): Flip image horizontally (left to right).\n        vertical_flip (<class 'bool'>): Flip image vertically (top to bottom).\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition.\n        reduce_mode (<class 'str'>): reduce mode when one text corresponds to multiple images in a chunk. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values"}, {"index": 97, "class_name": "image_watermark_filter", "class_desc": "Filter to keep samples whose images have no watermark with high probability.", "arguments": "        hf_watermark_model (<class 'str'>): watermark detection model name on huggingface.\n        trust_remote_code (<class 'bool'>): \n        prob_threshold (<class 'float'>): the predicted watermark probability threshold for samples. range from 0 to 1. Samples with watermark probability less than this threshold will be kept.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 98, "class_name": "language_id_score_filter", "class_desc": "Filter to keep samples in a specific language with confidence score larger than a specific min value.", "arguments": "        lang (typing.Union[str, typing.List[str]]): Samples in which languages to keep.\n        min_score (<class 'float'>): The min language identification confidence scores of samples to keep."}, {"index": 99, "class_name": "llm_quality_score_filter", "class_desc": "Filter to keep sample with high quality score estimated by LLM.", "arguments": "        api_or_hf_model (<class 'str'>): API or huggingface model name.\n        min_score (<class 'float'>): The lowest quality score threshold to keep the sample.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        input_keys (typing.List[str]): Sub set of keys in the sample. Support data with multi fields such as 'query', 'analysis' and 'answer' in RFT data.\n        field_names (typing.List[str]): Corresponding field names for input keys.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        field_template (typing.Optional[str]): Template for each field in the prompt.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        enable_vllm (<class 'bool'>): If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 100, "class_name": "llm_difficulty_score_filter", "class_desc": "Filter to keep sample with high difficulty score estimated by LLM.", "arguments": "        api_or_hf_model (<class 'str'>): API or huggingface model name.\n        min_score (<class 'float'>): The lowest difficulty score threshold to keep the sample.\n        api_endpoint (typing.Optional[str]): URL endpoint for the API.\n        response_path (typing.Optional[str]): Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n        input_keys (typing.List[str]): Sub set of keys in the sample. Support data with multi fields such as 'query', 'analysis' and 'answer' in RFT data.\n        field_names (typing.List[str]): Corresponding field names for input keys.\n        system_prompt (typing.Optional[str]): System prompt for the task.\n        input_template (typing.Optional[str]): Template for building the model input.\n        field_template (typing.Optional[str]): Template for each field in the prompt.\n        try_num (typing.Annotated[int, Gt(gt=0)]): The number of retry attempts when there is an API call error or output parsing error.\n        enable_vllm (<class 'bool'>): If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.\n        model_params (typing.Dict): Parameters for initializing the API model.\n        sampling_params (typing.Dict): Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}"}, {"index": 101, "class_name": "maximum_line_length_filter", "class_desc": "Filter to keep samples with maximum line length within a specific range.", "arguments": "        min_len (<class 'int'>): The min filter length in this op, samples will be filtered if their maximum line length is below this parameter.\n        max_len (<class 'int'>): The max filter length in this op, samples will be filtered if their maximum line length exceeds this parameter."}, {"index": 102, "class_name": "perplexity_filter", "class_desc": "Filter to keep samples with perplexity score less than a specific max value.", "arguments": "        lang (<class 'str'>): Compute perplexity for samples in which language.\n        max_ppl (<class 'float'>): The max filter perplexity in this op, samples will be filtered if their perplexity exceeds this parameter."}, {"index": 103, "class_name": "phrase_grounding_recall_filter", "class_desc": "Filter to keep samples whose locating recalls of phrases extracted from text in the images are within a specified range.", "arguments": "        hf_owlvit (<class 'str'>): Owl-ViT model name on huggingface to locate the phrases extracted from the text.\n        trust_remote_code (<class 'bool'>): \n        min_recall (<class 'float'>): The min phrase grounding recall to keep samples.\n        max_recall (<class 'float'>): The max phrase grounding recall to keep samples.\n        horizontal_flip (<class 'bool'>): Flip image horizontally (left to right).\n        vertical_flip (<class 'bool'>): Flip image vertically (top to bottom).\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition.\n        reduce_mode (<class 'str'>): reduce mode when one text corresponds to multiple images in a chunk. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values\n        iou_thr (<class 'float'>): the IoU threshold for NMS-like post-process. If two predicted bboxes are overlap with an IoU larger than this threshold, the bbox with less confidence will be removed. Default: 0.5.\n        large_area_ratio_thr (<class 'float'>): the area ratio threshold for filtering out those large predicted bboxes. If the area of a predicted bbox accounts for more than this ratio threshold of the whole image area, this bbox will be removed. Default: 0.95.\n        conf_thr (<class 'float'>): the confidence score threshold for removing low-confidence bboxes. If the confidence score of a predicted bbox is lower than the threshold, this bbox will be removed. Default: 0."}, {"index": 104, "class_name": "special_characters_filter", "class_desc": "Filter to keep samples with special-char ratio within a specific range.", "arguments": "        min_ratio (<class 'float'>): The min filter ratio in this op, samples will be filtered if their special-char ratio is below this parameter.\n        max_ratio (<class 'float'>): The max filter ratio in this op, samples will be filtered if their special-char ratio exceeds this parameter."}, {"index": 105, "class_name": "specified_field_filter", "class_desc": "Filter based on specified field information.  If the specified field information in the sample is not within the specified target value, the sample will be filtered.", "arguments": "        field_key (<class 'str'>): Filter based on the specified value corresponding to the target key. The target key corresponding to multi-level field information need to be separated by '.'.\n        target_value (typing.List): The range of specified field information corresponding to the samples that need to be retained."}, {"index": 106, "class_name": "specified_numeric_field_filter", "class_desc": "Filter based on specified numeric field information.  If the specified numeric information in the sample is not within the specified range, the sample will be filtered.", "arguments": "        field_key (<class 'str'>): Filter based on the specified numeric value corresponding to the target key. The target key corresponding to multi-level field information need to be separated by '.'.\n        min_value (<class 'float'>): The min filter value in SpecifiedNumericField op, samples will be filtered if their specified numeric field value is below this parameter.\n        max_value (<class 'float'>): The max filter value in SpecifiedNumericField op, samples will be filtered if their specified numeric field value exceeds this parameter."}, {"index": 107, "class_name": "stopwords_filter", "class_desc": "Filter to keep samples with stopword ratio larger than a specific min value.", "arguments": "        lang (<class 'str'>): Consider stopwords in what language. If lang == \"all\", we will adopt the one merged from all the available languages\n        tokenization (<class 'bool'>): whether to use model to tokenize documents\n        min_ratio (<class 'float'>): The min filter ratio in this op.\n        stopwords_dir (<class 'str'>): The directory storing the stopwords file(s) whose name includes \"stopwords\" and in json format\n        use_words_aug (<class 'bool'>): Whether to augment words, especially for Chinese and Vietnamese\n        words_aug_group_sizes (typing.List[typing.Annotated[int, Gt(gt=0)]]): The group size of words to augment\n        words_aug_join_char (<class 'str'>): The join char between words to augment"}, {"index": 108, "class_name": "suffix_filter", "class_desc": "Filter to keep samples with specified suffix.", "arguments": "        suffixes (typing.Union[str, typing.List[str]]): the suffix of text that will be keep. For example: '.txt', 'txt' or ['txt', '.pdf', 'docx']"}, {"index": 109, "class_name": "text_action_filter", "class_desc": "Filter to keep texts those contain actions in the text.", "arguments": "        lang (<class 'str'>): language of the text in the samples. 'en' for detection of actions in English and 'zh' for detection of actions in Chinese.\n        min_action_num (<class 'int'>): "}, {"index": 110, "class_name": "text_entity_dependency_filter", "class_desc": "Identify the entities in the text which are independent with other token, and filter them. The text containing no entities will be omitted.", "arguments": "        lang (<class 'str'>): language of the text in the samples. 'en' for detection of entities in English and 'zh' for detection of entities in Chinese.\n        min_dependency_num (<class 'int'>): \n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy. 'any': keep this sample if any object is dependent. 'all': keep this sample only if all images are dependent."}, {"index": 111, "class_name": "text_length_filter", "class_desc": "Filter to keep samples with total text length within a specific range.", "arguments": "        min_len (<class 'int'>): The min text length in the filtering. samples will be filtered if their text length is below this parameter.\n        max_len (<class 'int'>): The max text length in the filtering. samples will be filtered if their text length exceeds this parameter."}, {"index": 112, "class_name": "text_pair_similarity_filter", "class_desc": "Filter to keep text pairs with similarities between texts within a specific range.", "arguments": "        hf_clip (unknown): clip model name on huggingface to compute the similarity between image and text.\n        trust_remote_code (unknown): \n        min_score (<class 'jsonargparse.typing.ClosedUnitInterval'>): The min similarity to keep samples.\n        max_score (<class 'jsonargparse.typing.ClosedUnitInterval'>): The max similarity to keep samples.\n        text_key_second (unknown): used to store the other sentence in the text pair.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all images. 'any': keep this sample if any images meet the condition. 'all': keep this sample only if all images meet the condition."}, {"index": 113, "class_name": "token_num_filter", "class_desc": "Filter to keep samples with total token number within a specific range.", "arguments": "        hf_tokenizer (<class 'str'>): the tokenizer name of Hugging Face tokenizers.\n        min_num (<class 'int'>): The min filter token number in this op, samples will be filtered if their token number is below this parameter.\n        max_num (<class 'int'>): The max filter token number in this op, samples will be filtered if their token number exceeds this parameter."}, {"index": 114, "class_name": "video_aesthetics_filter", "class_desc": "Filter to keep data samples with aesthetics scores for specified frames in the videos within a specific range.", "arguments": "        hf_scorer_model (<class 'str'>): Huggingface model name for the aesthetics predictor. By default, we will use 'shunk031/aesthetics-predictor-v2-sac-logos-ava1-l14-linearMSE', refer to pypi.org/project/simple-aesthetics-predictor\n        trust_remote_code (<class 'bool'>): \n        min_score (<class 'float'>): Min score for the predicted aesthetics in a video.\n        max_score (<class 'float'>): Max score for the predicted aesthetics in a video.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame images from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames and the latter one extract specified number of frames uniformly from the video. Default: \"uniform\" with frame_num=3, considering that the number of keyframes can be large while their difference is usually small in terms of their aesthetics.\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        any_or_all (<class 'str'>): Keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition.\n        reduce_mode (<class 'str'>): reduce mode when one sample corresponds to multiple frames, must be one of ['avg','max', 'min']. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values"}, {"index": 115, "class_name": "video_aspect_ratio_filter", "class_desc": "Filter to keep samples with video aspect ratio within a specific range. AspectRatio = W / H.", "arguments": "        min_ratio (<class 'str'>): The minimum aspect ratio to keep samples, supported format is a string, such as \"9:21\" or \"9/21\".\n        max_ratio (<class 'str'>): The maximum aspect ratio to keep samples, supported format is a string, such as \"21:9\" or \"21/9\".\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 116, "class_name": "video_duration_filter", "class_desc": "Keep data samples whose videos' durations are within a specified range.", "arguments": "        min_duration (<class 'float'>): The min video duration to keep samples in seconds. It's 0 by default.\n        max_duration (<class 'float'>): The max video duration to keep samples in seconds. It's sys.maxsize by default.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 117, "class_name": "video_frames_text_similarity_filter", "class_desc": "Filter to keep samples those similarities between sampled video frame images and text within a specific range.", "arguments": "        hf_clip (unknown): clip model name on huggingface to compute the similarity between frame image and text. It's kind of language-related. For example, for Chinese datasets, ChineseCLIP might be a better choice.\n        trust_remote_code (unknown): \n        min_score (<class 'float'>): the min similarity to keep samples.\n        max_score (<class 'float'>): the max similarity to keep samples.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame images from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        horizontal_flip (<class 'bool'>): flip frame image horizontally (left to right).\n        vertical_flip (<class 'bool'>): flip frame image vertically (top to bottom).\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition.\n        reduce_mode (<class 'str'>): reduce mode when one text corresponds to multiple video frame images in a chunk. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values"}, {"index": 118, "class_name": "video_motion_score_filter", "class_desc": "Filter to keep samples with video motion scores within a specific range. The Farneback's algorithm from OpenCV is used to compute dense optical flow.", "arguments": "        min_score (<class 'float'>): The minimum motion score to keep samples.\n        max_score (<class 'float'>): The maximum motion score to keep samples.\n        sampling_fps (typing.Annotated[float, Gt(gt=0)]): The sampling rate in frames_per_second for optical flow calculations.\n        size (typing.Union[typing.Annotated[int, Gt(gt=0)], typing.Tuple[typing.Annotated[int, Gt(gt=0)]], typing.Tuple[typing.Annotated[int, Gt(gt=0)], typing.Annotated[int, Gt(gt=0)]], NoneType]): Resize frames before computing optical flow. If size is a sequence like (h, w), frame size will be matched to this. If size is an int, smaller edge of frames will be matched to this number. i.e, if height > width, then frame will be rescaled to (size * height / width, size). Default `None` to keep the original size.\n        max_size (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): The maximum allowed for the longer edge of resized frames. If the longer edge of frames is greater than max_size after being resized according to size, size will be overruled so that the longer edge is equal to max_size. As a result, the smaller edge may be shorter than size. This is only supported if size is an int.\n        divisible (typing.Annotated[int, Gt(gt=0)]): The number that the dimensions must be divisible by.\n        relative (<class 'bool'>): If `True`, the optical flow magnitude is normalized to a [0, 1] range, relative to the frame's diagonal length.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 119, "class_name": "video_motion_score_raft_filter", "class_desc": "Filter to keep samples with video motion scores within a specified range. This operator utilizes the RAFT (Recurrent All-Pairs Field Transforms) model from torchvision to predict optical flow between video frames.  For further details, refer to the official torchvision documentation: https://pytorch.org/vision/main/models/raft.html  The original paper on RAFT is available here: https://arxiv.org/abs/2003.12039", "arguments": "        min_score (<class 'float'>): The minimum motion score to keep samples.\n        max_score (<class 'float'>): The maximum motion score to keep samples.\n        sampling_fps (typing.Annotated[float, Gt(gt=0)]): The sampling rate in frames_per_second for optical flow calculations.\n        size (typing.Union[typing.Annotated[int, Gt(gt=0)], typing.Tuple[typing.Annotated[int, Gt(gt=0)]], typing.Tuple[typing.Annotated[int, Gt(gt=0)], typing.Annotated[int, Gt(gt=0)]], NoneType]): Resize frames before computing optical flow. If size is a sequence like (h, w), frame size will be matched to this. If size is an int, smaller edge of frames will be matched to this number. i.e, if height > width, then frame will be rescaled to (size * height / width, size). Default `None` to keep the original size.\n        max_size (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): The maximum allowed for the longer edge of resized frames. If the longer edge of frames is greater than max_size after being resized according to size, size will be overruled so that the longer edge is equal to max_size. As a result, the smaller edge may be shorter than size. This is only supported if size is an int.\n        divisible (typing.Annotated[int, Gt(gt=0)]): The number that the dimensions must be divisible by.\n        relative (<class 'bool'>): If `True`, the optical flow magnitude is normalized to a [0, 1] range, relative to the frame's diagonal length.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 120, "class_name": "video_nsfw_filter", "class_desc": "Filter to keep samples whose videos have low nsfw scores.", "arguments": "        hf_nsfw_model (<class 'str'>): nsfw detection model name on huggingface.\n        trust_remote_code (<class 'bool'>): \n        max_score (<class 'float'>): the nsfw score threshold for samples. range from 0 to 1. Samples with nsfw score less than this threshold will be kept.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame images from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        reduce_mode (<class 'str'>): reduce mode for multiple sampled video frames. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 121, "class_name": "video_ocr_area_ratio_filter", "class_desc": "Keep data samples whose detected text area ratios for specified frames in the video are within a specified range.", "arguments": "        min_area_ratio (<class 'float'>): The min ocr area ratio to keep samples. It's 0 by default.\n        max_area_ratio (<class 'float'>): The max ocr area ratio to keep samples. It's 1.0 by default.\n        frame_sample_num (typing.Annotated[int, Gt(gt=0)]): The number of sampled frames to calculate the ocr area ratio. If it's 1, only middle frame will be selected. If it's 2, only the first and the last frames will be selected. If it's larger than 2, in addition to the first and the last frames, other frames will be sampled evenly within the video duration.\n        languages_to_detect (typing.Union[str, typing.List[str]]): texts in which languages should be detected. Default: ['ch_sim', 'en']. Full language list can be found here: https://www.jaided.ai/easyocr/.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 122, "class_name": "video_resolution_filter", "class_desc": "Keep data samples whose videos' resolutions are within a specified range.", "arguments": "        min_width (<class 'int'>): The min horizontal resolution.\n        max_width (<class 'int'>): The max horizontal resolution.\n        min_height (<class 'int'>): The min vertical resolution.\n        max_height (<class 'int'>): The max vertical resolution.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 123, "class_name": "video_tagging_from_frames_filter", "class_desc": "Filter to keep samples whose videos contain the given tags.", "arguments": "        tags (typing.List[str]): a tag list to shift the videos, total tags can be found in https://github.com/xinyu1205/recognize-anything/blob/main/ram/data/ram_tag_list.txt # noqa: E501\n        contain (<class 'str'>): require the videos containing 'any' or 'all' tags. When tags equal to [], 'all' keeps all samples, 'any' keeps no sample.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame images from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        tag_field_name (<class 'str'>): the key name to store the tags in the meta field. It's \"video_frame_tags\" in default.\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 124, "class_name": "video_watermark_filter", "class_desc": "Filter to keep samples whose videos have no watermark with high probability.", "arguments": "        hf_watermark_model (<class 'str'>): watermark detection model name on huggingface.\n        trust_remote_code (<class 'bool'>): \n        prob_threshold (<class 'float'>): the predicted watermark probability threshold for samples. range from 0 to 1. Samples with watermark probability less than this threshold will be kept.\n        frame_sampling_method (<class 'str'>): sampling method of extracting frame images from the videos. Should be one of [\"all_keyframes\", \"uniform\"]. The former one extracts all key frames (the number of which depends on the duration of the video) and the latter one extract specified number of frames uniformly from the video. Default: \"all_keyframes\".\n        frame_num (typing.Annotated[int, Gt(gt=0)]): the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is \"uniform\". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.\n        reduce_mode (<class 'str'>): reduce mode for multiple sampled video frames. 'avg': Take the average of multiple values 'max': Take the max of multiple values 'min': Take the min of multiple values\n        any_or_all (<class 'str'>): keep this sample with 'any' or 'all' strategy of all videos. 'any': keep this sample if any videos meet the condition. 'all': keep this sample only if all videos meet the condition."}, {"index": 125, "class_name": "word_repetition_filter", "class_desc": "Filter to keep samples with word-level n-gram repetition ratio within a specific range.", "arguments": "        lang (<class 'str'>): sample in which language.\n        tokenization (<class 'bool'>): whether to use model to tokenize documents\n        rep_len (typing.Annotated[int, Gt(gt=0)]): Repetition length for word-level n-gram.\n        min_ratio (<class 'float'>): The min filter ratio in this op, samples will be filtered if their word-level n-gram repetition ratio is below this parameter.\n        max_ratio (<class 'float'>): The max filter ratio in this op, samples will be filtered if their word-level n-gram repetition ratio exceeds this parameter."}, {"index": 126, "class_name": "words_num_filter", "class_desc": "Filter to keep samples with total words number within a specific range.", "arguments": "        lang (<class 'str'>): sample in which language.\n        tokenization (<class 'bool'>): whether to use model to tokenize documents\n        min_num (<class 'int'>): The min filter word number in this op, samples will be filtered if their word number is below this parameter.\n        max_num (<class 'int'>): The max filter word number in this op, samples will be filtered if their word number exceeds this parameter."}, {"index": 127, "class_name": "document_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching.  Using md5 hash to deduplicate samples.", "arguments": "        lowercase (<class 'bool'>): Whether to convert sample text to lower case\n        ignore_non_character (<class 'bool'>): Whether to ignore non-alphabet characters, including whitespaces, digits, and punctuations"}, {"index": 128, "class_name": "document_minhash_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using MinHashLSH.  Different from simhash, minhash is stored as bytes, so they won't be kept in the final dataset.", "arguments": "        tokenization (<class 'str'>): tokenization method for sample texts. It should be one of [space, punctuation, character, sentencepiece]. For English-like languages, we recommend to use 'space', for Chinese-like languages, we recommend to use 'character', and for multiple languages, we recommend to use 'sentencepiece'. If using 'sentencepiece', please provided the model path in the 'tokenizer_model' field.\n        window_size (typing.Annotated[int, Gt(gt=0)]): window size of shingling\n        lowercase (<class 'bool'>): whether to convert text to lower case first\n        ignore_pattern (typing.Optional[str]): whether to ignore sub-strings with specific pattern when computing minhash\n        num_permutations (typing.Annotated[int, Gt(gt=0)]): number of permutations in minhash computing\n        jaccard_threshold (typing.Annotated[float, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=0), Le(le=1)])]): the min jaccard similarity threshold in near-duplicate detection. When the jaccard similarity of two sample texts is >= this threshold, they are regarded as similar samples and this op will only keep one of them after deduplication\n        num_bands (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): number of bands in LSH. Default it's None, and it will be determined by an optimal params computation algorithm by minimize the weighted sum of probs of False Positives and False Negatives\n        num_rows_per_band (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): number of rows in each band in LSH. Default it's None, and it will be determined by an optimal params computation algorithm\n        tokenizer_model (typing.Optional[str]): path for the sentencepiece model, used for sentencepiece tokenization."}, {"index": 129, "class_name": "document_simhash_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using SimHash.", "arguments": "        tokenization (<class 'str'>): \n        window_size (typing.Annotated[int, Gt(gt=0)]): window size of shingling\n        lowercase (<class 'bool'>): whether to convert text to lower case first\n        ignore_pattern (typing.Optional[str]): whether to ignore sub-strings with specific pattern when computing simhash\n        num_blocks (typing.Annotated[int, Gt(gt=0)]): number of blocks in simhash computing\n        hamming_distance (typing.Annotated[int, Gt(gt=0)]): the max hamming distance threshold in near-duplicate detection. When the hamming distance of two sample texts is <= this threshold, they are regarded as similar samples and this op will only keep one of them after deduplication. This threshold should be always less than num_blocks"}, {"index": 130, "class_name": "image_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching of images between documents.", "arguments": "        method (<class 'str'>): hash method for image\n        consider_text (<class 'bool'>): whether to consider text hash together with image hash when applying deduplication."}, {"index": 131, "class_name": "ray_document_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching.", "arguments": "        backend (<class 'str'>): the backend for dedup, either 'ray_actor' or 'redis'\n        redis_address (<class 'str'>): the address of redis server\n        lowercase (<class 'bool'>): Whether to convert sample text to lower case\n        ignore_non_character (<class 'bool'>): Whether to ignore non-alphabet characters, including whitespaces, digits, and punctuations"}, {"index": 132, "class_name": "ray_image_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching of images between documents.", "arguments": "        backend (<class 'str'>): the backend for dedup, either 'ray_actor' or 'redis'\n        redis_address (<class 'str'>): the address of redis server\n        method (<class 'str'>): "}, {"index": 133, "class_name": "ray_video_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching of videos between documents.", "arguments": "        backend (<class 'str'>): the backend for dedup, either 'ray_actor' or 'redis'\n        redis_address (<class 'str'>): the address of redis server"}, {"index": 134, "class_name": "ray_image_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching of images between documents.", "arguments": "        backend (<class 'str'>): the backend for dedup, either 'ray_actor' or 'redis'\n        redis_address (<class 'str'>): the address of redis server\n        method (<class 'str'>): "}, {"index": 135, "class_name": "ray_bts_minhash_deduplicator", "class_desc": "A MinhashLSH deduplicator based on RAY.", "arguments": "        tokenization (<class 'str'>): tokenization method for sample texts. It should be one of [space, punctuation, character, sentencepiece]. For English-like languages, we recommend to use 'space', for Chinese-like languages, we recommend to use 'character', and for multiple languages, we recommend to use 'sentencepiece'. If using 'sentencepiece', please provided the model path in the 'tokenizer_model' field.\n        window_size (typing.Annotated[int, Gt(gt=0)]): window size of shingling\n        lowercase (<class 'bool'>): whether to convert text to lower case first\n        ignore_pattern (typing.Optional[str]): whether to ignore sub-strings with specific pattern when computing minhash\n        num_permutations (typing.Annotated[int, Gt(gt=0)]): number of permutations in minhash computing\n        jaccard_threshold (typing.Annotated[float, FieldInfo(annotation=NoneType, required=True, metadata=[Ge(ge=0), Le(le=1)])]): the min jaccard similarity threshold in near-duplicate detection. When the jaccard similarity of two sample texts is >= this threshold, they are regarded as similar samples and this op will only keep one of them after deduplication\n        num_bands (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): number of bands in LSH. Default it's None, and it will be determined by an optimal params computation algorithm by minimize the weighted sum of probs of False Positives and False Negatives\n        num_rows_per_band (typing.Optional[typing.Annotated[int, Gt(gt=0)]]): number of rows in each band in LSH. Default it's None, and it will be determined by an optimal params computation algorithm\n        tokenizer_model (typing.Optional[str]): path for the sentencepiece model, used for sentencepiece tokenization.\n        union_find_parallel_num (typing.Union[int, str]): number of parallel workers for union-find algorithm. Default it's 'auto', and it will be determined by half of the number of CPUs.\n        union_threshold (typing.Optional[int]): threshold for minhash values group to perform union-find algorithm. Default it's 256.\n        max_pending_edge_buffer_task (typing.Optional[int]): max number of pending edge buffer ray tasks. Default it's 20.\n        num_edge_buffer_task_returns (typing.Optional[int]): number of edge buffer tasks for `ray.wait` to return. Default it's 10.\n        max_pending_filter_tasks (typing.Optional[int]): max number of pending filter ray tasks. Default it's 20.\n        num_filter_task_returns (typing.Optional[int]): number of filter tasks for `ray.wait` to return. Default it's 10.\n        merge_batch_size (typing.Optional[int]): batch size for BTS operations. Default it's 1000."}, {"index": 136, "class_name": "video_deduplicator", "class_desc": "Deduplicator to deduplicate samples at document-level using exact matching of videos between documents.", "arguments": "        consider_text (<class 'bool'>): whether to consider text hash together with video hash when applying deduplication."}]