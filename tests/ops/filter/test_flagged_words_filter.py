import unittest

from datasets import Dataset

from data_juicer.ops.filter.flagged_words_filter import FlaggedWordFilter
from data_juicer.utils.constant import Fields


class FlaggedWordFilterTest(unittest.TestCase):

    def _run_flagged_words_filter(self, dataset: Dataset, target_list, op):
        if Fields.stats not in dataset.features:
            # TODO:
            # this is a temp solution,
            # only add stats when calling filter op
            dataset = dataset.add_column(name=Fields.stats,
                                         column=[{}] * dataset.num_rows)
        dataset = dataset.map(op.compute_stats)
        dataset = dataset.filter(op.process)
        dataset = dataset.select_columns(column_names=['text'])
        res_list = dataset.to_list()
        self.assertEqual(res_list, target_list)

    def test_en_case(self):

        ds_list = [{
            'text': 'Today is anal cumshot day'
        }, {
            'text': 'Fuck you doggystyle!'
        }, {
            'text': 'ï¼Œã€‚ã€â€â€â€œÂ«Â»ï¼‘ã€ã€Œã€Šã€‹Â´âˆ¶ï¼šï¼Ÿï¼ï¼ˆï¼‰ï¼›â€“â€”ï¼ï½â€™â€¦â”ã€ˆã€‰ã€ã€‘ï¼…â–º'
        }, {
            'text': 'Do you need a cup of coffee?'
        }, {
            'text': 'emojiè¡¨æƒ…æµ‹è¯•ä¸‹ğŸ˜Šï¼ŒğŸ˜¸31231\n'
        }]
        tgt_list = [{
            'text': 'ï¼Œã€‚ã€â€â€â€œÂ«Â»ï¼‘ã€ã€Œã€Šã€‹Â´âˆ¶ï¼šï¼Ÿï¼ï¼ˆï¼‰ï¼›â€“â€”ï¼ï½â€™â€¦â”ã€ˆã€‰ã€ã€‘ï¼…â–º'
        }, {
            'text': 'Do you need a cup of coffee?'
        }, {
            'text': 'emojiè¡¨æƒ…æµ‹è¯•ä¸‹ğŸ˜Šï¼ŒğŸ˜¸31231\n'
        }]
        dataset = Dataset.from_list(ds_list)
        op = FlaggedWordFilter(lang='en', max_ratio=0.045)
        self._run_flagged_words_filter(dataset, tgt_list, op)

    def test_zh_case(self):

        ds_list = [{
            'text': 'ä½ æ˜¯ä¸ªå–æ·«å¥³'
        }, {
            'text': 'æ ¹æ®ç®—å­ä½¿ç”¨æƒ…å†µå¢é‡å®‰è£…æ–¹æ¡ˆç¡®å®š'
        }, {
            'text': 'å»é™¤å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å æ¯”è¿‡ä½æˆ–è¿‡é«˜çš„ä»£ç '
        }, {
            'text': 'åŸºäºå‰ä¸€æ­¥ç»“æœï¼Œé™¤æ‰æ‰“é£æœºã€ä¸‰çº§ç‰‡ç­‰æ•æ„Ÿè¯'
        }, {
            'text': 'ä½¿ç”¨ç‰‡æ®µåˆ†è¯å™¨å¯¹æ¯ä¸ªé¡µé¢è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹è®¡ç®—æ¯ä¸ªæ®µè½çš„å›°æƒ‘åº¦å¾—åˆ†ï¼Œç”±æ­¤è¿‡æ»¤ä½è´¨é‡æ–‡æœ¬'
        }]
        tgt_list = [{
            'text': 'æ ¹æ®ç®—å­ä½¿ç”¨æƒ…å†µå¢é‡å®‰è£…æ–¹æ¡ˆç¡®å®š'
        }, {
            'text': 'å»é™¤å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å æ¯”è¿‡ä½æˆ–è¿‡é«˜çš„ä»£ç '
        }, {
            'text': 'ä½¿ç”¨ç‰‡æ®µåˆ†è¯å™¨å¯¹æ¯ä¸ªé¡µé¢è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹è®¡ç®—æ¯ä¸ªæ®µè½çš„å›°æƒ‘åº¦å¾—åˆ†ï¼Œç”±æ­¤è¿‡æ»¤ä½è´¨é‡æ–‡æœ¬'
        }]
        dataset = Dataset.from_list(ds_list)
        op = FlaggedWordFilter(lang='zh',
                               tokenization=True,
                               max_ratio=0.045,
                               use_words_aug=True)
        self._run_flagged_words_filter(dataset, tgt_list, op)


if __name__ == '__main__':
    unittest.main()
