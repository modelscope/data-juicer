# MindGYM

Implementation of the paper _**MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?**_.

> Large foundation models face challenges in acquiring transferable, structured thinking abilities, especially when supervised with rigid templates or crowd-annotated instruction datasets. Unlike prior approaches, we focus on a thinking-centric data synthesis paradigm that enables models to evolve through self-generated, cognitively guided data. We propose MindGYM, a structured and scalable framework for question synthesis, composed of: (1) Cognitive Thinking Process Injection, which infuses high-level reasoning objectives to shape the modelâ€™s synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating atomic questions from diverse semantic types to encourage broader thinking; and (3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop questions based on QA seeds for deeper reasoning. 
Detailed analysis shows that synthetic data generated by our method achieves 16.7\% higher average quality and 67.91\% lower quality variance compared to baseline sources, highlighting that both high-quality and self-contained data are essential for effective, thinking-oriented finetuning. 
MindGYM improves performance on six reasoning benchmarks, achieving gains of up to 16\% on MathVision using only 400 data samples, and generalizable improvements across different model sizes and architectures. 
MindGYM underscores the viability of self-challenging mechanisms in refining large model capabilities while minimizing human intervention and resource demands.
Code and data are released to promote data-centric research into self-evolving foundation models driven by their internal reasoning capabilities.

![image](https://github.com/user-attachments/assets/805a3359-e07c-4088-8316-2d7993bd6b76)


_**The width- and breadth-based scoring and self-challenging synthesis OPs are being incorporated into Data-Juicer main branch.**_

The models and datasets used in the paper are as follows: 

From Huggingface: 
[Qwen2.5-VL-7B](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)
[ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA)
[OK-VQA](https://huggingface.co/datasets/lmms-lab/OK-VQA)

From Modelscope:
[Qwen2.5-VL-7B](https://modelscope.cn/models/Qwen/Qwen2.5-VL-7B-Instruct)

## Step 0. Installation

### Conda Environment

```bash
conda create -n mindgym python=3.10
```

### Pip Installation

```bash
pip install -r requirements.txt
```

## Step 1. Self-challenging QAs Generation

Data synthesis is divided into text and image ends, both of which are implemented in the form of pipline, and only one call is needed to synthesize data. The file is located in `./data_generate`, which can realize data synthesis in Chinese, English, text, and images. We integrated _Seed Single-Hop Question Synthesis_ and _Challenging Multi-Hop Question Synthesis_ into one pipeline.

For text data:
```bash
bash data_generate.sh
bash data_generate_cn.sh
```

For image data (taking ScienceQA as an example):
```bash
bash data_generate_img.sh
bash data_generate_img_cn.sh
```

## Step 2. TrainFormatPrepare

After generating data using the code in `data_generate`, the data needs to be preprocessed. The file is located in `./data_process` to adapt to the training data format of [ms-swift](https://github.com/modelscope/ms-swift).

For text data:
```python
python ./data_process/process.py --input_en_path \[input_file_en\] --input_cn_path [input_file_cn]
```

For image data (note that the path of the image needs to be modified accordingly):
```python
python ./data_process/process_img.py --input_en_path \[input_file_en\] --input_cn_path [input_file_cn]
```

The following eight training data for course learning will be obtained (taking text data as an example):

- _Question + Thinking -> Answer_
- _Question + Answer -> Thinking_
- _Question + Answer -> Thinking_
- _Question -> Answer_

![image](https://github.com/user-attachments/assets/a2967eae-d783-4db3-b494-3400526aeacd)

## Step 3. Thinking-Induced Curriculum Fine-Tuning

Curriculum Learning: Training is divided into four steps

_Question + Thinking -> Answer_: Corresponding to the above `cn_with_0306_0.json`

```bash
bash ./train/step-1.sh
```

_Question + Answer -> Thinking_: Corresponding to the above `cn_with_0306_1.json`

```bash
bash ./train/step-2.sh
```

_Question + Answer -> Thinking_: Corresponding to the above `cn_with_0306_2.json`

```bash
bash ./train/step-3.sh
```

_Question -> Answer_: Corresponding to the above `cn_with_0306_3.json`

```bash
bash ./train/step-4.sh
```

For image data, you need to set the `--freeze_aligner` parameter in the `bash` file to `False` to align the text model and the visual model.

## Step 4. The Implementation of Eval Module

For the detailed implementation of evaluation, please refer to `./eval`.

### Installation

To ensure the latest capabilities of ms-swift are used, it is recommended to install from the GitHub repository.

1. **Conda Environment:**

```bash
conda create -n ms-eval python=3.10
```

2. **Install from source:**

```bash
git clone https://github.com/modelscope/ms-swift.git
cd ms-swift
pip install -e '.[eval]'
```

3. **If replication is needed, the core dependencies are:**

```bash
python=3.10, evalscope==0.11.0, ms-swift==3.2.0, ms-vlmeval==0.0.13, torch==2.5.1, transformers==4.49.0
```

For detailed `pip list`, please refer to `./eval/requirements.txt`. Note that an additional library that may need to be installed is `qwen-vl-utils` for Qwen2.5-VL, and `timm` for Intern2.5-VL.

### Evaluation

We have listed the detailed evaluation scripts in this folder, including the Qwen2.5-VL series and Intern2.5-VL series. As an example, we focus on the evaluation script for Qwen2.5-VL-7B-Instruct located in the `./eval/qw25/` folder.

1. **Multimodal Evaluation**

We selected four datasets: `MMMU`, `MMStar`, `MathVision`, and `MathVista`, to conduct comprehensive evaluations. The `--eval_backend`is set to `VLMEvalKit`. Except for `MMMU` (`./eval/qw25/mm-eval-2.sh`), where we evaluate only 500 samples, we conduct full evaluations on the other datasets (`./eval/qw25/mm-eval-1.sh`), resulting in two separate scripts. Note that the`OPENAI_API_KEY` can be used directly, but **remember to delete it when pushing to an anonymous repo**.

2. **Text-based Evaluation**

We selected `AIME24`, `GSM8K`, `MATH`, `TriviaQA`, and `GPQA` for comprehensive evaluation. The `--eval_backend` is set to `Native`. Similarly, except for `GPQA` (`./eval/qw25/text-eval-2.sh`), where we set a limit of 300 due to time constraints, we conduct full evaluations on the other datasets (`./eval/qw25/text-eval-1.sh`), resulting in two separate scripts.

(To avoid repetitive operations, can we ignore the time constraints and perform a full evaluation? It would take an additional 2-3 hours approximately.)

Two points to note:

- Keep `eval_num_proc` set to `1`. Increasing the value will only increase the number of ports used without speeding up the process, and too many ports may cause port conflicts and self-locking.
- The `--port` setting: Ports are theoretically allocated automatically, but if you need to interrupt in the middle, uncleared ports might cause conflicts. Therefore, it is recommended to set the ports manually.

### Results

![results](https://github.com/user-attachments/assets/0e78a844-787e-4a1e-ac5b-e2a530b29f41)

To validate the efficacy of our methodology, we systematically evaluate the model across multiple benchmarks as documented in the Table 1.

![data-juicer](https://github.com/user-attachments/assets/62086699-e561-4c16-9ca7-fcefd538f7e5)

To comprehensively assess the quality of our MindGYM synthetic dataset, we adopt Data-Juicer, a modular data-centric analysis toolkit that provides LLM-guided operators for probing data across various dimensions. Specifically, we compare MindGYM against several baselines (e.g., LIMO, Open-O1, MMEvol) using five diagnostic filters: (1) quality, which estimates the overall textual quality based on LLM judgment; (2) action, counting the number of action verbs as a proxy for instruction richness; (3) dependency, which penalizes the presence of non-independent noun phrases based on syntactic trees; (4) token, evaluating token-length consistency; and (5) length, which considers raw text length variability. Table 2 summarizes the full results across multiple model series and checkpoints.

